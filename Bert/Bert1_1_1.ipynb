{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bng0z-EfaGdr",
    "outputId": "ab5a4d3a-02fa-4887-d47e-b01a140a638b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\programdata\\anaconda3\\lib\\site-packages (4.29.2)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (21.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: filelock in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (3.3.1)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (0.13.3)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (0.14.1)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (1.20.3)\n",
      "Requirement already satisfied: requests in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (2.26.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (4.62.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (2021.8.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\programdata\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (3.10.0.2)\n",
      "Requirement already satisfied: fsspec in c:\\programdata\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2021.10.1)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from packaging>=20.0->transformers) (3.0.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\youngchan kim\\appdata\\roaming\\python\\python39\\site-packages (from tqdm>=4.27->transformers) (0.4.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->transformers) (1.26.7)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->transformers) (3.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->transformers) (2021.10.8)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -umpy (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -umpy (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -umpy (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -umpy (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -umpy (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -umpy (c:\\programdata\\anaconda3\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tabulate in c:\\programdata\\anaconda3\\lib\\site-packages (0.9.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -umpy (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -umpy (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -umpy (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -umpy (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -umpy (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -umpy (c:\\programdata\\anaconda3\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "!pip install tabulate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "8GDw68IjZzAW"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from tabulate import tabulate\n",
    "from tqdm import trange\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm, tqdm_notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 316
    },
    "id": "UglczWslaIK5",
    "outputId": "e2337623-2d02-4e71-d39f-3d1d22104e5b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label       int64\n",
      "url        object\n",
      "rating      int64\n",
      "date       object\n",
      "content    object\n",
      "dtype: object\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>url</th>\n",
       "      <th>rating</th>\n",
       "      <th>date</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>/review/rw1142613/</td>\n",
       "      <td>6</td>\n",
       "      <td>4 August 2005</td>\n",
       "      <td>It's an \"emotional manipulation for dummies\" t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>/review/rw1587268/</td>\n",
       "      <td>6</td>\n",
       "      <td>30 January 2007</td>\n",
       "      <td>Tim Robbins plays Andy Dufresne, a man convict...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>/review/rw0349436/</td>\n",
       "      <td>6</td>\n",
       "      <td>18 December 2003</td>\n",
       "      <td>It is the most basic of all principles of film...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>/review/rw0349359/</td>\n",
       "      <td>6</td>\n",
       "      <td>30 July 2003</td>\n",
       "      <td>Little more than average drama for popular tas...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>/review/rw0348197/</td>\n",
       "      <td>6</td>\n",
       "      <td>10 September 1998</td>\n",
       "      <td>An UNDER-RATED movie?? Gimme a break! This is ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                 url  rating               date  \\\n",
       "0      0  /review/rw1142613/       6      4 August 2005   \n",
       "1      0  /review/rw1587268/       6    30 January 2007   \n",
       "2      0  /review/rw0349436/       6   18 December 2003   \n",
       "3      0  /review/rw0349359/       6       30 July 2003   \n",
       "4      0  /review/rw0348197/       6  10 September 1998   \n",
       "\n",
       "                                             content  \n",
       "0  It's an \"emotional manipulation for dummies\" t...  \n",
       "1  Tim Robbins plays Andy Dufresne, a man convict...  \n",
       "2  It is the most basic of all principles of film...  \n",
       "3  Little more than average drama for popular tas...  \n",
       "4  An UNDER-RATED movie?? Gimme a break! This is ...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# file_path = './drive/MyDrive/movie_reviews.xlsx'\n",
    "file_path = './movie_reviews.xlsx'\n",
    "df = pd.read_excel(file_path)\n",
    "print(df.dtypes)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wc6gLWFChRDg",
    "outputId": "f25030a2-0e81-4151-aa4b-be5fcc115654"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\Youngchan\n",
      "[nltk_data]     Kim\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "5sL5ilC1mnFS"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1. 전체 문장'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''1. 전체 문장'''\n",
    "# text = df.content.values\n",
    "# labels = df.label.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "EbUlxK2VcRae"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 3.96 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "'''2. 두문장 씩 묶어서'''\n",
    "text = []\n",
    "labels = []\n",
    "content = df.content.values\n",
    "temp = df.label.values\n",
    "for i, sentence in enumerate(content):\n",
    "    if temp[i] >= 50: \n",
    "        break\n",
    "    try:\n",
    "        # print(content)\n",
    "        sentences = sent_tokenize(str(sentence))\n",
    "        for j in range(0, len(sentences)):\n",
    "            pair = sentences[j]+sentences[j + 1] if j + 1 < len(sentences) else sentences[j]\n",
    "            text.append(pair)\n",
    "            labels.append(temp[i])\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6XkG3-vokj6H",
    "outputId": "2c496d2d-e796-4ccc-a3e6-9285e2baa7a7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "148367"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1H2ZSvQ7klr8",
    "outputId": "77272990-6ea2-46b8-8194-2cda94358bac"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "148367"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "DnFaJitvcbqj"
   },
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(\n",
    "    'bert-base-multilingual-uncased',\n",
    "    do_lower_case = True # True/모두 소문자로 변환. False: 대소문자 구분\n",
    "    )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "71g6AH6De0gn"
   },
   "source": [
    "bert-base-uncased  \n",
    "bert-base-cased  \n",
    "bert-base-multilingual-uncased  \n",
    "bert-base-multilingual-cased  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pW9ziJundlXT",
    "outputId": "156cd42d-f0f0-43b1-a666-27fb5f2c9317"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I watched the extended version of Ridley Scott's Gladiator, and despite my low interest on films related to Medieval era, I was simply mesmerized by its sheer magnitude, its energy, and the protagonist's (Russel Crowe) audacity to fight back in the face of failure, all of which could only be topped by the almost perfect casting of Joaquin Phoenix, his eyes, and his incredible performance as the brutal and henpecked emperor of the falling Roman empire.TN.\n",
      "╒═════════════╤═════════════╕\n",
      "│ Tokens      │   Token IDs │\n",
      "╞═════════════╪═════════════╡\n",
      "│ i           │         151 │\n",
      "├─────────────┼─────────────┤\n",
      "│ watched     │       84447 │\n",
      "├─────────────┼─────────────┤\n",
      "│ the         │       10103 │\n",
      "├─────────────┼─────────────┤\n",
      "│ extended    │       19164 │\n",
      "├─────────────┼─────────────┤\n",
      "│ version     │       10947 │\n",
      "├─────────────┼─────────────┤\n",
      "│ of          │       10108 │\n",
      "├─────────────┼─────────────┤\n",
      "│ ridley      │       56140 │\n",
      "├─────────────┼─────────────┤\n",
      "│ scott       │       13064 │\n",
      "├─────────────┼─────────────┤\n",
      "│ '           │         112 │\n",
      "├─────────────┼─────────────┤\n",
      "│ s           │         161 │\n",
      "├─────────────┼─────────────┤\n",
      "│ glad        │       73327 │\n",
      "├─────────────┼─────────────┤\n",
      "│ ##iator     │       85730 │\n",
      "├─────────────┼─────────────┤\n",
      "│ ,           │         117 │\n",
      "├─────────────┼─────────────┤\n",
      "│ and         │       10110 │\n",
      "├─────────────┼─────────────┤\n",
      "│ despite     │       15738 │\n",
      "├─────────────┼─────────────┤\n",
      "│ my          │       11153 │\n",
      "├─────────────┼─────────────┤\n",
      "│ low         │       14298 │\n",
      "├─────────────┼─────────────┤\n",
      "│ interest    │       17079 │\n",
      "├─────────────┼─────────────┤\n",
      "│ on          │       10125 │\n",
      "├─────────────┼─────────────┤\n",
      "│ films       │       13076 │\n",
      "├─────────────┼─────────────┤\n",
      "│ related     │       16038 │\n",
      "├─────────────┼─────────────┤\n",
      "│ to          │       10114 │\n",
      "├─────────────┼─────────────┤\n",
      "│ medieval    │       17620 │\n",
      "├─────────────┼─────────────┤\n",
      "│ era         │       10420 │\n",
      "├─────────────┼─────────────┤\n",
      "│ ,           │         117 │\n",
      "├─────────────┼─────────────┤\n",
      "│ i           │         151 │\n",
      "├─────────────┼─────────────┤\n",
      "│ was         │       10140 │\n",
      "├─────────────┼─────────────┤\n",
      "│ simply      │       24817 │\n",
      "├─────────────┼─────────────┤\n",
      "│ mes         │       11019 │\n",
      "├─────────────┼─────────────┤\n",
      "│ ##meri      │       91782 │\n",
      "├─────────────┼─────────────┤\n",
      "│ ##zed       │       17807 │\n",
      "├─────────────┼─────────────┤\n",
      "│ by          │       10151 │\n",
      "├─────────────┼─────────────┤\n",
      "│ its         │       10491 │\n",
      "├─────────────┼─────────────┤\n",
      "│ she         │       10572 │\n",
      "├─────────────┼─────────────┤\n",
      "│ ##er        │       10177 │\n",
      "├─────────────┼─────────────┤\n",
      "│ magnitude   │       30190 │\n",
      "├─────────────┼─────────────┤\n",
      "│ ,           │         117 │\n",
      "├─────────────┼─────────────┤\n",
      "│ its         │       10491 │\n",
      "├─────────────┼─────────────┤\n",
      "│ energy      │       14828 │\n",
      "├─────────────┼─────────────┤\n",
      "│ ,           │         117 │\n",
      "├─────────────┼─────────────┤\n",
      "│ and         │       10110 │\n",
      "├─────────────┼─────────────┤\n",
      "│ the         │       10103 │\n",
      "├─────────────┼─────────────┤\n",
      "│ protagonist │       55476 │\n",
      "├─────────────┼─────────────┤\n",
      "│ '           │         112 │\n",
      "├─────────────┼─────────────┤\n",
      "│ s           │         161 │\n",
      "├─────────────┼─────────────┤\n",
      "│ (           │         113 │\n",
      "├─────────────┼─────────────┤\n",
      "│ russe       │       23755 │\n",
      "├─────────────┼─────────────┤\n",
      "│ ##l         │       10159 │\n",
      "├─────────────┼─────────────┤\n",
      "│ crow        │       46049 │\n",
      "├─────────────┼─────────────┤\n",
      "│ ##e         │       10111 │\n",
      "├─────────────┼─────────────┤\n",
      "│ )           │         114 │\n",
      "├─────────────┼─────────────┤\n",
      "│ au          │       10257 │\n",
      "├─────────────┼─────────────┤\n",
      "│ ##da        │       10250 │\n",
      "├─────────────┼─────────────┤\n",
      "│ ##city      │       48940 │\n",
      "├─────────────┼─────────────┤\n",
      "│ to          │       10114 │\n",
      "├─────────────┼─────────────┤\n",
      "│ fight       │       17260 │\n",
      "├─────────────┼─────────────┤\n",
      "│ back        │       11677 │\n",
      "├─────────────┼─────────────┤\n",
      "│ in          │       10104 │\n",
      "├─────────────┼─────────────┤\n",
      "│ the         │       10103 │\n",
      "├─────────────┼─────────────┤\n",
      "│ face        │       12828 │\n",
      "├─────────────┼─────────────┤\n",
      "│ of          │       10108 │\n",
      "├─────────────┼─────────────┤\n",
      "│ failure     │       28350 │\n",
      "├─────────────┼─────────────┤\n",
      "│ ,           │         117 │\n",
      "├─────────────┼─────────────┤\n",
      "│ all         │       10367 │\n",
      "├─────────────┼─────────────┤\n",
      "│ of          │       10108 │\n",
      "├─────────────┼─────────────┤\n",
      "│ which       │       10359 │\n",
      "├─────────────┼─────────────┤\n",
      "│ could       │       12296 │\n",
      "├─────────────┼─────────────┤\n",
      "│ only        │       10902 │\n",
      "├─────────────┼─────────────┤\n",
      "│ be          │       10346 │\n",
      "├─────────────┼─────────────┤\n",
      "│ topped      │       81916 │\n",
      "├─────────────┼─────────────┤\n",
      "│ by          │       10151 │\n",
      "├─────────────┼─────────────┤\n",
      "│ the         │       10103 │\n",
      "├─────────────┼─────────────┤\n",
      "│ almost      │       16398 │\n",
      "├─────────────┼─────────────┤\n",
      "│ perfect     │       23021 │\n",
      "├─────────────┼─────────────┤\n",
      "│ casting     │       44726 │\n",
      "├─────────────┼─────────────┤\n",
      "│ of          │       10108 │\n",
      "├─────────────┼─────────────┤\n",
      "│ joaquin     │       28843 │\n",
      "├─────────────┼─────────────┤\n",
      "│ phoenix     │       21073 │\n",
      "├─────────────┼─────────────┤\n",
      "│ ,           │         117 │\n",
      "├─────────────┼─────────────┤\n",
      "│ his         │       10235 │\n",
      "├─────────────┼─────────────┤\n",
      "│ eyes        │       19452 │\n",
      "├─────────────┼─────────────┤\n",
      "│ ,           │         117 │\n",
      "├─────────────┼─────────────┤\n",
      "│ and         │       10110 │\n",
      "├─────────────┼─────────────┤\n",
      "│ his         │       10235 │\n",
      "├─────────────┼─────────────┤\n",
      "│ incredible  │       81981 │\n",
      "├─────────────┼─────────────┤\n",
      "│ performance │       13512 │\n",
      "├─────────────┼─────────────┤\n",
      "│ as          │       10146 │\n",
      "├─────────────┼─────────────┤\n",
      "│ the         │       10103 │\n",
      "├─────────────┼─────────────┤\n",
      "│ brutal      │       63573 │\n",
      "├─────────────┼─────────────┤\n",
      "│ and         │       10110 │\n",
      "├─────────────┼─────────────┤\n",
      "│ hen         │       21223 │\n",
      "├─────────────┼─────────────┤\n",
      "│ ##pec       │       56478 │\n",
      "├─────────────┼─────────────┤\n",
      "│ ##ked       │       33190 │\n",
      "├─────────────┼─────────────┤\n",
      "│ emperor     │       19293 │\n",
      "├─────────────┼─────────────┤\n",
      "│ of          │       10108 │\n",
      "├─────────────┼─────────────┤\n",
      "│ the         │       10103 │\n",
      "├─────────────┼─────────────┤\n",
      "│ falling     │       26640 │\n",
      "├─────────────┼─────────────┤\n",
      "│ roman       │       11458 │\n",
      "├─────────────┼─────────────┤\n",
      "│ empire      │       13359 │\n",
      "├─────────────┼─────────────┤\n",
      "│ .           │         119 │\n",
      "├─────────────┼─────────────┤\n",
      "│ tn          │       42906 │\n",
      "├─────────────┼─────────────┤\n",
      "│ .           │         119 │\n",
      "╘═════════════╧═════════════╛\n"
     ]
    }
   ],
   "source": [
    "def print_rand_sentence():\n",
    "    '''Displays the tokens and respective IDs of a random text sample'''\n",
    "    index = random.randint(0, len(text)-1)\n",
    "    print(text[index])\n",
    "    table = np.array([tokenizer.tokenize(text[index]),\n",
    "                      tokenizer.convert_tokens_to_ids(tokenizer.tokenize(text[index]))]).T\n",
    "    print(tabulate(table, headers = ['Tokens', 'Token IDs'], tablefmt = 'fancy_grid'))\n",
    "\n",
    "print_rand_sentence()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "j7tJh_fNlk3v",
    "outputId": "1d53872f-fd15-4fa6-9e82-520bddd9d0e7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It's an \"emotional manipulation for dummies\" type of film, with boring direction and cardboard characters.Nice performances by the actors though, some great moments, but nothing outside the \"Hallmark meets Stephen King meets Prison\" feel of the entire movie.I saw it one and a half times, and would see it again only if I got paid for the trouble.Similarly, I hated 'The Green Mile\", only later did I find out it's the work of Darabont.My recommendation: Skip this, and go see OZ again.\n",
      "╒══════════════╤═════════════╕\n",
      "│ Tokens       │   Token IDs │\n",
      "╞══════════════╪═════════════╡\n",
      "│ it           │       10197 │\n",
      "├──────────────┼─────────────┤\n",
      "│ '            │         112 │\n",
      "├──────────────┼─────────────┤\n",
      "│ s            │         161 │\n",
      "├──────────────┼─────────────┤\n",
      "│ an           │       10144 │\n",
      "├──────────────┼─────────────┤\n",
      "│ \"            │         107 │\n",
      "├──────────────┼─────────────┤\n",
      "│ emotional    │       48740 │\n",
      "├──────────────┼─────────────┤\n",
      "│ mani         │       34016 │\n",
      "├──────────────┼─────────────┤\n",
      "│ ##pulation   │       88298 │\n",
      "├──────────────┼─────────────┤\n",
      "│ for          │       10139 │\n",
      "├──────────────┼─────────────┤\n",
      "│ dum          │       26728 │\n",
      "├──────────────┼─────────────┤\n",
      "│ ##mies       │       48789 │\n",
      "├──────────────┼─────────────┤\n",
      "│ \"            │         107 │\n",
      "├──────────────┼─────────────┤\n",
      "│ type         │       12340 │\n",
      "├──────────────┼─────────────┤\n",
      "│ of           │       10108 │\n",
      "├──────────────┼─────────────┤\n",
      "│ film         │       10388 │\n",
      "├──────────────┼─────────────┤\n",
      "│ ,            │         117 │\n",
      "├──────────────┼─────────────┤\n",
      "│ with         │       10171 │\n",
      "├──────────────┼─────────────┤\n",
      "│ bor          │       22001 │\n",
      "├──────────────┼─────────────┤\n",
      "│ ##ing        │       10285 │\n",
      "├──────────────┼─────────────┤\n",
      "│ direction    │       14821 │\n",
      "├──────────────┼─────────────┤\n",
      "│ and          │       10110 │\n",
      "├──────────────┼─────────────┤\n",
      "│ card         │       18579 │\n",
      "├──────────────┼─────────────┤\n",
      "│ ##board      │       29527 │\n",
      "├──────────────┼─────────────┤\n",
      "│ characters   │       18751 │\n",
      "├──────────────┼─────────────┤\n",
      "│ .            │         119 │\n",
      "├──────────────┼─────────────┤\n",
      "│ nice         │       24242 │\n",
      "├──────────────┼─────────────┤\n",
      "│ performances │       22187 │\n",
      "├──────────────┼─────────────┤\n",
      "│ by           │       10151 │\n",
      "├──────────────┼─────────────┤\n",
      "│ the          │       10103 │\n",
      "├──────────────┼─────────────┤\n",
      "│ actors       │       26826 │\n",
      "├──────────────┼─────────────┤\n",
      "│ though       │       14325 │\n",
      "├──────────────┼─────────────┤\n",
      "│ ,            │         117 │\n",
      "├──────────────┼─────────────┤\n",
      "│ some         │       10970 │\n",
      "├──────────────┼─────────────┤\n",
      "│ great        │       11838 │\n",
      "├──────────────┼─────────────┤\n",
      "│ moments      │       36723 │\n",
      "├──────────────┼─────────────┤\n",
      "│ ,            │         117 │\n",
      "├──────────────┼─────────────┤\n",
      "│ but          │       10502 │\n",
      "├──────────────┼─────────────┤\n",
      "│ nothing      │       20587 │\n",
      "├──────────────┼─────────────┤\n",
      "│ outside      │       16751 │\n",
      "├──────────────┼─────────────┤\n",
      "│ the          │       10103 │\n",
      "├──────────────┼─────────────┤\n",
      "│ \"            │         107 │\n",
      "├──────────────┼─────────────┤\n",
      "│ hall         │       11672 │\n",
      "├──────────────┼─────────────┤\n",
      "│ ##mark       │       22824 │\n",
      "├──────────────┼─────────────┤\n",
      "│ meets        │       31587 │\n",
      "├──────────────┼─────────────┤\n",
      "│ stephen      │       14235 │\n",
      "├──────────────┼─────────────┤\n",
      "│ king         │       11479 │\n",
      "├──────────────┼─────────────┤\n",
      "│ meets        │       31587 │\n",
      "├──────────────┼─────────────┤\n",
      "│ prison       │       18357 │\n",
      "├──────────────┼─────────────┤\n",
      "│ \"            │         107 │\n",
      "├──────────────┼─────────────┤\n",
      "│ feel         │       23333 │\n",
      "├──────────────┼─────────────┤\n",
      "│ of           │       10108 │\n",
      "├──────────────┼─────────────┤\n",
      "│ the          │       10103 │\n",
      "├──────────────┼─────────────┤\n",
      "│ entire       │       19401 │\n",
      "├──────────────┼─────────────┤\n",
      "│ movie        │       13113 │\n",
      "├──────────────┼─────────────┤\n",
      "│ .            │         119 │\n",
      "├──────────────┼─────────────┤\n",
      "│ i            │         151 │\n",
      "├──────────────┼─────────────┤\n",
      "│ saw          │       16289 │\n",
      "├──────────────┼─────────────┤\n",
      "│ it           │       10197 │\n",
      "├──────────────┼─────────────┤\n",
      "│ one          │       10399 │\n",
      "├──────────────┼─────────────┤\n",
      "│ and          │       10110 │\n",
      "├──────────────┼─────────────┤\n",
      "│ a            │         143 │\n",
      "├──────────────┼─────────────┤\n",
      "│ half         │       13460 │\n",
      "├──────────────┼─────────────┤\n",
      "│ times        │       11471 │\n",
      "├──────────────┼─────────────┤\n",
      "│ ,            │         117 │\n",
      "├──────────────┼─────────────┤\n",
      "│ and          │       10110 │\n",
      "├──────────────┼─────────────┤\n",
      "│ would        │       11008 │\n",
      "├──────────────┼─────────────┤\n",
      "│ see          │       11811 │\n",
      "├──────────────┼─────────────┤\n",
      "│ it           │       10197 │\n",
      "├──────────────┼─────────────┤\n",
      "│ again        │       12590 │\n",
      "├──────────────┼─────────────┤\n",
      "│ only         │       10902 │\n",
      "├──────────────┼─────────────┤\n",
      "│ if           │       11526 │\n",
      "├──────────────┼─────────────┤\n",
      "│ i            │         151 │\n",
      "├──────────────┼─────────────┤\n",
      "│ got          │       15517 │\n",
      "├──────────────┼─────────────┤\n",
      "│ paid         │       25033 │\n",
      "├──────────────┼─────────────┤\n",
      "│ for          │       10139 │\n",
      "├──────────────┼─────────────┤\n",
      "│ the          │       10103 │\n",
      "├──────────────┼─────────────┤\n",
      "│ trouble      │       29868 │\n",
      "├──────────────┼─────────────┤\n",
      "│ .            │         119 │\n",
      "├──────────────┼─────────────┤\n",
      "│ similarly    │       45535 │\n",
      "├──────────────┼─────────────┤\n",
      "│ ,            │         117 │\n",
      "├──────────────┼─────────────┤\n",
      "│ i            │         151 │\n",
      "├──────────────┼─────────────┤\n",
      "│ hate         │       39487 │\n",
      "├──────────────┼─────────────┤\n",
      "│ ##d          │       10163 │\n",
      "├──────────────┼─────────────┤\n",
      "│ '            │         112 │\n",
      "├──────────────┼─────────────┤\n",
      "│ the          │       10103 │\n",
      "├──────────────┼─────────────┤\n",
      "│ green        │       12535 │\n",
      "├──────────────┼─────────────┤\n",
      "│ mile         │       17623 │\n",
      "├──────────────┼─────────────┤\n",
      "│ \"            │         107 │\n",
      "├──────────────┼─────────────┤\n",
      "│ ,            │         117 │\n",
      "├──────────────┼─────────────┤\n",
      "│ only         │       10902 │\n",
      "├──────────────┼─────────────┤\n",
      "│ later        │       10844 │\n",
      "├──────────────┼─────────────┤\n",
      "│ did          │       12266 │\n",
      "├──────────────┼─────────────┤\n",
      "│ i            │         151 │\n",
      "├──────────────┼─────────────┤\n",
      "│ find         │       16595 │\n",
      "├──────────────┼─────────────┤\n",
      "│ out          │       10871 │\n",
      "├──────────────┼─────────────┤\n",
      "│ it           │       10197 │\n",
      "├──────────────┼─────────────┤\n",
      "│ '            │         112 │\n",
      "├──────────────┼─────────────┤\n",
      "│ s            │         161 │\n",
      "├──────────────┼─────────────┤\n",
      "│ the          │       10103 │\n",
      "├──────────────┼─────────────┤\n",
      "│ work         │       11497 │\n",
      "├──────────────┼─────────────┤\n",
      "│ of           │       10108 │\n",
      "├──────────────┼─────────────┤\n",
      "│ darab        │       79729 │\n",
      "├──────────────┼─────────────┤\n",
      "│ ##ont        │       20923 │\n",
      "├──────────────┼─────────────┤\n",
      "│ .            │         119 │\n",
      "├──────────────┼─────────────┤\n",
      "│ my           │       11153 │\n",
      "├──────────────┼─────────────┤\n",
      "│ rec          │       44909 │\n",
      "├──────────────┼─────────────┤\n",
      "│ ##ommen      │       55667 │\n",
      "├──────────────┼─────────────┤\n",
      "│ ##dation     │       77393 │\n",
      "├──────────────┼─────────────┤\n",
      "│ :            │         131 │\n",
      "├──────────────┼─────────────┤\n",
      "│ skip         │       38825 │\n",
      "├──────────────┼─────────────┤\n",
      "│ this         │       10372 │\n",
      "├──────────────┼─────────────┤\n",
      "│ ,            │         117 │\n",
      "├──────────────┼─────────────┤\n",
      "│ and          │       10110 │\n",
      "├──────────────┼─────────────┤\n",
      "│ go           │       11335 │\n",
      "├──────────────┼─────────────┤\n",
      "│ see          │       11811 │\n",
      "├──────────────┼─────────────┤\n",
      "│ oz           │       17704 │\n",
      "├──────────────┼─────────────┤\n",
      "│ again        │       12590 │\n",
      "├──────────────┼─────────────┤\n",
      "│ .            │         119 │\n",
      "╘══════════════╧═════════════╛\n"
     ]
    }
   ],
   "source": [
    "def print_sentence(index):\n",
    "    print(text[index])\n",
    "    table = np.array([tokenizer.tokenize(text[index]),\n",
    "                      tokenizer.convert_tokens_to_ids(tokenizer.tokenize(text[index]))]).T\n",
    "    print(tabulate(table, headers = ['Tokens', 'Token IDs'], tablefmt = 'fancy_grid'))\n",
    "\n",
    "print_sentence(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NAn7gRGVeNR5",
    "outputId": "439bdd1c-eec1-40c9-c0f5-887dc929d1e0"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2364: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 2min 22s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "token_id = []\n",
    "attention_masks = []\n",
    "\n",
    "def preprocessing(input_text, tokenizer):\n",
    "  '''\n",
    "  Returns <class transformers.tokenization_utils_base.BatchEncoding> with the following fields:\n",
    "    - input_ids: list of token ids\n",
    "    - token_type_ids: list of token type ids\n",
    "    - attention_mask: list of indices (0,1) specifying which tokens should considered by the model (return_attention_mask = True).\n",
    "  '''\n",
    "  return tokenizer.encode_plus(\n",
    "                        input_text,\n",
    "                        add_special_tokens = True,\n",
    "                        max_length = 128,\n",
    "                        pad_to_max_length = True,\n",
    "                        return_attention_mask = True,\n",
    "                        return_tensors = 'pt'\n",
    "                   )\n",
    "\n",
    "\n",
    "for sample in text:\n",
    "    encoding_dict = preprocessing(sample, tokenizer)\n",
    "    token_id.append(encoding_dict['input_ids']) \n",
    "    attention_masks.append(encoding_dict['attention_mask'])\n",
    "\n",
    "token_id = torch.cat(token_id, dim = 0)\n",
    "attention_masks = torch.cat(attention_masks, dim = 0)\n",
    "labels = torch.tensor(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AnF5OF0PfaiB",
    "outputId": "583ca210-b47d-427e-ec1e-1387e344a3f9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  101, 24242, 22187, 10151, 10103, 26826, 14325,   117, 10970, 11838,\n",
       "        36723,   117, 10502, 20587, 16751, 10103,   107, 11672, 22824, 31587,\n",
       "        14235, 11479, 31587, 18357,   107, 23333, 10108, 10103, 19401, 13113,\n",
       "          119,   151, 16289, 10197, 10399, 10110,   143, 13460, 11471,   117,\n",
       "        10110, 11008, 11811, 10197, 12590, 10902, 11526,   151, 15517, 25033,\n",
       "        10139, 10103, 29868,   119, 45535,   117,   151, 39487, 10163,   112,\n",
       "        10103, 12535, 17623,   107,   117, 10902, 10844, 12266,   151, 16595,\n",
       "        10871, 10197,   112,   161, 10103, 11497, 10108, 79729, 20923,   119,\n",
       "        11153, 44909, 55667, 77393,   131, 38825, 10372,   117, 10110, 11335,\n",
       "        11811, 17704, 12590,   119,   102,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_id[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jASZmzkfnD-7",
    "outputId": "7623dc5d-f79f-45a5-f5e8-bd59007eb693"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "╒═══════════════╤═════════════╤══════════════════╕\n",
      "│ Tokens        │   Token IDs │   Attention Mask │\n",
      "╞═══════════════╪═════════════╪══════════════════╡\n",
      "│ [CLS]         │         101 │                1 │\n",
      "├───────────────┼─────────────┼──────────────────┤\n",
      "│ martial       │       37363 │                1 │\n",
      "├───────────────┼─────────────┼──────────────────┤\n",
      "│ arts          │       12180 │                1 │\n",
      "├───────────────┼─────────────┼──────────────────┤\n",
      "│ skills        │       30504 │                1 │\n",
      "├───────────────┼─────────────┼──────────────────┤\n",
      "│ ,             │         117 │                1 │\n",
      "├───────────────┼─────────────┼──────────────────┤\n",
      "│ cun           │       35962 │                1 │\n",
      "├───────────────┼─────────────┼──────────────────┤\n",
      "│ ##ning        │       11459 │                1 │\n",
      "├───────────────┼─────────────┼──────────────────┤\n",
      "│ ,             │         117 │                1 │\n",
      "├───────────────┼─────────────┼──────────────────┤\n",
      "│ great         │       11838 │                1 │\n",
      "├───────────────┼─────────────┼──────────────────┤\n",
      "│ tactical      │       50293 │                1 │\n",
      "├───────────────┼─────────────┼──────────────────┤\n",
      "│ thinking      │       38138 │                1 │\n",
      "├───────────────┼─────────────┼──────────────────┤\n",
      "│ ,             │         117 │                1 │\n",
      "├───────────────┼─────────────┼──────────────────┤\n",
      "│ foren         │       31952 │                1 │\n",
      "├───────────────┼─────────────┼──────────────────┤\n",
      "│ ##sic         │       45426 │                1 │\n",
      "├───────────────┼─────────────┼──────────────────┤\n",
      "│ application   │       18529 │                1 │\n",
      "├───────────────┼─────────────┼──────────────────┤\n",
      "│ ,             │         117 │                1 │\n",
      "├───────────────┼─────────────┼──────────────────┤\n",
      "│ technological │       71901 │                1 │\n",
      "├───────────────┼─────────────┼──────────────────┤\n",
      "│ genius        │       52586 │                1 │\n",
      "├───────────────┼─────────────┼──────────────────┤\n",
      "│ to            │       10114 │                1 │\n",
      "├───────────────┼─────────────┼──────────────────┤\n",
      "│ advance       │       25590 │                1 │\n",
      "├───────────────┼─────────────┼──────────────────┤\n",
      "│ or            │       10362 │                1 │\n",
      "├───────────────┼─────────────┼──────────────────┤\n",
      "│ improve       │       32922 │                1 │\n",
      "├───────────────┼─────────────┼──────────────────┤\n",
      "│ lu            │       12993 │                1 │\n",
      "├───────────────┼─────────────┼──────────────────┤\n",
      "│ ##scio        │       74260 │                1 │\n",
      "├───────────────┼─────────────┼──────────────────┤\n",
      "│ ##us          │       10258 │                1 │\n",
      "├───────────────┼─────────────┼──────────────────┤\n",
      "│ fox           │       14530 │                1 │\n",
      "├───────────────┼─────────────┼──────────────────┤\n",
      "│ '             │         112 │                1 │\n",
      "├───────────────┼─────────────┼──────────────────┤\n",
      "│ s             │         161 │                1 │\n",
      "├───────────────┼─────────────┼──────────────────┤\n",
      "│ invention     │       46318 │                1 │\n",
      "├───────────────┼─────────────┼──────────────────┤\n",
      "│ ##s           │       10107 │                1 │\n",
      "├───────────────┼─────────────┼──────────────────┤\n",
      "│ /             │         120 │                1 │\n",
      "├───────────────┼─────────────┼──────────────────┤\n",
      "│ technological │       71901 │                1 │\n",
      "├───────────────┼─────────────┼──────────────────┤\n",
      "│ breakthrough  │       85628 │                1 │\n",
      "├───────────────┼─────────────┼──────────────────┤\n",
      "│ ##s           │       10107 │                1 │\n",
      "├───────────────┼─────────────┼──────────────────┤\n",
      "│ ,             │         117 │                1 │\n",
      "├───────────────┼─────────────┼──────────────────┤\n",
      "│ inti          │       82038 │                1 │\n",
      "├───────────────┼─────────────┼──────────────────┤\n",
      "│ ##mida        │       49727 │                1 │\n",
      "├───────────────┼─────────────┼──────────────────┤\n",
      "│ ##ting        │       12305 │                1 │\n",
      "├───────────────┼─────────────┼──────────────────┤\n",
      "│ personality   │       35148 │                1 │\n",
      "├───────────────┼─────────────┼──────────────────┤\n",
      "│ ,             │         117 │                1 │\n",
      "├───────────────┼─────────────┼──────────────────┤\n",
      "│ and           │       10110 │                1 │\n",
      "├───────────────┼─────────────┼──────────────────┤\n",
      "│ even          │       12818 │                1 │\n",
      "├───────────────┼─────────────┼──────────────────┤\n",
      "│ a             │         143 │                1 │\n",
      "├───────────────┼─────────────┼──────────────────┤\n",
      "│ little        │       11975 │                1 │\n",
      "├───────────────┼─────────────┼──────────────────┤\n",
      "│ sw            │       17158 │                1 │\n",
      "├───────────────┼─────────────┼──────────────────┤\n",
      "│ ##ash         │       62931 │                1 │\n",
      "├───────────────┼─────────────┼──────────────────┤\n",
      "│ ##bu          │       12217 │                1 │\n",
      "├───────────────┼─────────────┼──────────────────┤\n",
      "│ ##ck          │       11732 │                1 │\n",
      "├───────────────┼─────────────┼──────────────────┤\n",
      "│ ##ling        │       12024 │                1 │\n",
      "├───────────────┼─────────────┼──────────────────┤\n",
      "│ .             │         119 │                1 │\n",
      "├───────────────┼─────────────┼──────────────────┤\n",
      "│ as            │       10146 │                1 │\n",
      "├───────────────┼─────────────┼──────────────────┤\n",
      "│ for           │       10139 │                1 │\n",
      "├───────────────┼─────────────┼──────────────────┤\n",
      "│ heath         │       39121 │                1 │\n",
      "├───────────────┼─────────────┼──────────────────┤\n",
      "│ ,             │         117 │                1 │\n",
      "├───────────────┼─────────────┼──────────────────┤\n",
      "│ yes           │       31617 │                1 │\n",
      "├───────────────┼─────────────┼──────────────────┤\n",
      "│ he            │       10191 │                1 │\n",
      "├───────────────┼─────────────┼──────────────────┤\n",
      "│ gets          │       25917 │                1 │\n",
      "├───────────────┼─────────────┼──────────────────┤\n",
      "│ credit        │       23814 │                1 │\n",
      "├───────────────┼─────────────┼──────────────────┤\n",
      "│ for           │       10139 │                1 │\n",
      "├───────────────┼─────────────┼──────────────────┤\n",
      "│ his           │       10235 │                1 │\n",
      "├───────────────┼─────────────┼──────────────────┤\n",
      "│ performance   │       13512 │                1 │\n",
      "├───────────────┼─────────────┼──────────────────┤\n",
      "│ as            │       10146 │                1 │\n",
      "├───────────────┼─────────────┼──────────────────┤\n",
      "│ the           │       10103 │                1 │\n",
      "├───────────────┼─────────────┼──────────────────┤\n",
      "│ joker         │       58892 │                1 │\n",
      "├───────────────┼─────────────┼──────────────────┤\n",
      "│ .             │         119 │                1 │\n",
      "├───────────────┼─────────────┼──────────────────┤\n",
      "│ [SEP]         │         102 │                1 │\n",
      "├───────────────┼─────────────┼──────────────────┤\n",
      "│ [PAD]         │           0 │                0 │\n",
      "├───────────────┼─────────────┼──────────────────┤\n",
      "│ [PAD]         │           0 │                0 │\n",
      "├───────────────┼─────────────┼──────────────────┤\n",
      "│ [PAD]         │           0 │                0 │\n",
      "├───────────────┼─────────────┼──────────────────┤\n",
      "│ [PAD]         │           0 │                0 │\n",
      "├───────────────┼─────────────┼──────────────────┤\n",
      "│ [PAD]         │           0 │                0 │\n",
      "├───────────────┼─────────────┼──────────────────┤\n",
      "│ [PAD]         │           0 │                0 │\n",
      "├───────────────┼─────────────┼──────────────────┤\n",
      "│ [PAD]         │           0 │                0 │\n",
      "├───────────────┼─────────────┼──────────────────┤\n",
      "│ [PAD]         │           0 │                0 │\n",
      "├───────────────┼─────────────┼──────────────────┤\n",
      "│ [PAD]         │           0 │                0 │\n",
      "├───────────────┼─────────────┼──────────────────┤\n",
      "│ [PAD]         │           0 │                0 │\n",
      "├───────────────┼─────────────┼──────────────────┤\n",
      "│ [PAD]         │           0 │                0 │\n",
      "├───────────────┼─────────────┼──────────────────┤\n",
      "│ [PAD]         │           0 │                0 │\n",
      "├───────────────┼─────────────┼──────────────────┤\n",
      "│ [PAD]         │           0 │                0 │\n",
      "├───────────────┼─────────────┼──────────────────┤\n",
      "│ [PAD]         │           0 │                0 │\n",
      "├───────────────┼─────────────┼──────────────────┤\n",
      "│ [PAD]         │           0 │                0 │\n",
      "├───────────────┼─────────────┼──────────────────┤\n",
      "│ [PAD]         │           0 │                0 │\n",
      "├───────────────┼─────────────┼──────────────────┤\n",
      "│ [PAD]         │           0 │                0 │\n",
      "├───────────────┼─────────────┼──────────────────┤\n",
      "│ [PAD]         │           0 │                0 │\n",
      "├───────────────┼─────────────┼──────────────────┤\n",
      "│ [PAD]         │           0 │                0 │\n",
      "├───────────────┼─────────────┼──────────────────┤\n",
      "│ [PAD]         │           0 │                0 │\n",
      "├───────────────┼─────────────┼──────────────────┤\n",
      "│ [PAD]         │           0 │                0 │\n",
      "├───────────────┼─────────────┼──────────────────┤\n",
      "│ [PAD]         │           0 │                0 │\n",
      "├───────────────┼─────────────┼──────────────────┤\n",
      "│ [PAD]         │           0 │                0 │\n",
      "├───────────────┼─────────────┼──────────────────┤\n",
      "│ [PAD]         │           0 │                0 │\n",
      "├───────────────┼─────────────┼──────────────────┤\n",
      "│ [PAD]         │           0 │                0 │\n",
      "├───────────────┼─────────────┼──────────────────┤\n",
      "│ [PAD]         │           0 │                0 │\n",
      "├───────────────┼─────────────┼──────────────────┤\n",
      "│ [PAD]         │           0 │                0 │\n",
      "├───────────────┼─────────────┼──────────────────┤\n",
      "│ [PAD]         │           0 │                0 │\n",
      "├───────────────┼─────────────┼──────────────────┤\n",
      "│ [PAD]         │           0 │                0 │\n",
      "├───────────────┼─────────────┼──────────────────┤\n",
      "│ [PAD]         │           0 │                0 │\n",
      "├───────────────┼─────────────┼──────────────────┤\n",
      "│ [PAD]         │           0 │                0 │\n",
      "├───────────────┼─────────────┼──────────────────┤\n",
      "│ [PAD]         │           0 │                0 │\n",
      "├───────────────┼─────────────┼──────────────────┤\n",
      "│ [PAD]         │           0 │                0 │\n",
      "├───────────────┼─────────────┼──────────────────┤\n",
      "│ [PAD]         │           0 │                0 │\n",
      "├───────────────┼─────────────┼──────────────────┤\n",
      "│ [PAD]         │           0 │                0 │\n",
      "├───────────────┼─────────────┼──────────────────┤\n",
      "│ [PAD]         │           0 │                0 │\n",
      "├───────────────┼─────────────┼──────────────────┤\n",
      "│ [PAD]         │           0 │                0 │\n",
      "├───────────────┼─────────────┼──────────────────┤\n",
      "│ [PAD]         │           0 │                0 │\n",
      "├───────────────┼─────────────┼──────────────────┤\n",
      "│ [PAD]         │           0 │                0 │\n",
      "├───────────────┼─────────────┼──────────────────┤\n",
      "│ [PAD]         │           0 │                0 │\n",
      "├───────────────┼─────────────┼──────────────────┤\n",
      "│ [PAD]         │           0 │                0 │\n",
      "├───────────────┼─────────────┼──────────────────┤\n",
      "│ [PAD]         │           0 │                0 │\n",
      "├───────────────┼─────────────┼──────────────────┤\n",
      "│ [PAD]         │           0 │                0 │\n",
      "├───────────────┼─────────────┼──────────────────┤\n",
      "│ [PAD]         │           0 │                0 │\n",
      "├───────────────┼─────────────┼──────────────────┤\n",
      "│ [PAD]         │           0 │                0 │\n",
      "├───────────────┼─────────────┼──────────────────┤\n",
      "│ [PAD]         │           0 │                0 │\n",
      "├───────────────┼─────────────┼──────────────────┤\n",
      "│ [PAD]         │           0 │                0 │\n",
      "├───────────────┼─────────────┼──────────────────┤\n",
      "│ [PAD]         │           0 │                0 │\n",
      "├───────────────┼─────────────┼──────────────────┤\n",
      "│ [PAD]         │           0 │                0 │\n",
      "├───────────────┼─────────────┼──────────────────┤\n",
      "│ [PAD]         │           0 │                0 │\n",
      "├───────────────┼─────────────┼──────────────────┤\n",
      "│ [PAD]         │           0 │                0 │\n",
      "├───────────────┼─────────────┼──────────────────┤\n",
      "│ [PAD]         │           0 │                0 │\n",
      "├───────────────┼─────────────┼──────────────────┤\n",
      "│ [PAD]         │           0 │                0 │\n",
      "├───────────────┼─────────────┼──────────────────┤\n",
      "│ [PAD]         │           0 │                0 │\n",
      "├───────────────┼─────────────┼──────────────────┤\n",
      "│ [PAD]         │           0 │                0 │\n",
      "├───────────────┼─────────────┼──────────────────┤\n",
      "│ [PAD]         │           0 │                0 │\n",
      "├───────────────┼─────────────┼──────────────────┤\n",
      "│ [PAD]         │           0 │                0 │\n",
      "├───────────────┼─────────────┼──────────────────┤\n",
      "│ [PAD]         │           0 │                0 │\n",
      "├───────────────┼─────────────┼──────────────────┤\n",
      "│ [PAD]         │           0 │                0 │\n",
      "├───────────────┼─────────────┼──────────────────┤\n",
      "│ [PAD]         │           0 │                0 │\n",
      "├───────────────┼─────────────┼──────────────────┤\n",
      "│ [PAD]         │           0 │                0 │\n",
      "├───────────────┼─────────────┼──────────────────┤\n",
      "│ [PAD]         │           0 │                0 │\n",
      "╘═══════════════╧═════════════╧══════════════════╛\n",
      "Wall time: 8.97 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def print_rand_sentence_encoding():\n",
    "  '''Displays tokens, token IDs and attention mask of a random text sample'''\n",
    "  index = random.randint(0, len(text) - 1)\n",
    "  tokens = tokenizer.tokenize(tokenizer.decode(token_id[index]))\n",
    "  token_ids = [i.numpy() for i in token_id[index]]\n",
    "  attention = [i.numpy() for i in attention_masks[index]]\n",
    "\n",
    "  table = np.array([tokens, token_ids, attention]).T\n",
    "  print(tabulate(table, headers = ['Tokens', 'Token IDs', 'Attention Mask'], tablefmt = 'fancy_grid'))\n",
    "\n",
    "print_rand_sentence_encoding()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "eGBNmXMZnGya"
   },
   "outputs": [],
   "source": [
    "val_ratio = 0.25\n",
    "batch_size = 16\n",
    "\n",
    "# Indices of the train and validation splits stratified by labels\n",
    "train_idx, val_idx = train_test_split(\n",
    "    np.arange(len(labels)),\n",
    "    test_size = val_ratio,\n",
    "    shuffle = True,\n",
    "    stratify = labels)\n",
    "\n",
    "# Train and validation sets\n",
    "train_set = TensorDataset(token_id[train_idx], \n",
    "                          attention_masks[train_idx], \n",
    "                          labels[train_idx])\n",
    "\n",
    "val_set = TensorDataset(token_id[val_idx], \n",
    "                        attention_masks[val_idx], \n",
    "                        labels[val_idx])\n",
    "\n",
    "# Prepare DataLoader\n",
    "train_dataloader = DataLoader(\n",
    "            train_set,\n",
    "            sampler = RandomSampler(train_set),\n",
    "            batch_size = batch_size\n",
    "        )\n",
    "\n",
    "validation_dataloader = DataLoader(\n",
    "            val_set,\n",
    "            sampler = SequentialSampler(val_set),\n",
    "            batch_size = batch_size\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "ozF5dCmUphm_"
   },
   "outputs": [],
   "source": [
    "def b_tp(preds, labels):\n",
    "    '''Returns True Positives (TP): count of correct predictions of actual class 1'''\n",
    "    return sum([preds == labels and preds == 1 for preds, labels in zip(preds, labels)])\n",
    "\n",
    "def b_fp(preds, labels):\n",
    "    '''Returns False Positives (FP): count of wrong predictions of actual class 1'''\n",
    "    return sum([preds != labels and preds == 1 for preds, labels in zip(preds, labels)])\n",
    "\n",
    "def b_tn(preds, labels):\n",
    "    '''Returns True Negatives (TN): count of correct predictions of actual class 0'''\n",
    "    return sum([preds == labels and preds == 0 for preds, labels in zip(preds, labels)])\n",
    "\n",
    "def b_fn(preds, labels):\n",
    "    '''Returns False Negatives (FN): count of wrong predictions of actual class 0'''\n",
    "    return sum([preds != labels and preds == 0 for preds, labels in zip(preds, labels)])\n",
    "\n",
    "def b_metrics(preds, labels):\n",
    "    '''\n",
    "    Returns the following metrics:\n",
    "        - accuracy    = (TP + TN) / N\n",
    "        - precision   = TP / (TP + FP)\n",
    "        - recall      = TP / (TP + FN)\n",
    "        - specificity = TN / (TN + FP)\n",
    "    '''\n",
    "    preds = np.argmax(preds, axis = 1).flatten()\n",
    "    labels = labels.flatten()\n",
    "    tp = b_tp(preds, labels)\n",
    "    tn = b_tn(preds, labels)\n",
    "    fp = b_fp(preds, labels)\n",
    "    fn = b_fn(preds, labels)\n",
    "    b_accuracy = (tp + tn) / len(labels)\n",
    "    b_precision = tp / (tp + fp) if (tp + fp) > 0 else 'nan'\n",
    "    b_recall = tp / (tp + fn) if (tp + fn) > 0 else 'nan'\n",
    "    b_specificity = tn / (tn + fp) if (tn + fp) > 0 else 'nan'\n",
    "    return b_accuracy, b_precision, b_recall, b_specificity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2vRSUOt4pnDh",
    "outputId": "51aca52c-ad55-43ba-ee7c-2e371d3920b5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1.9 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(105879, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=50, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    'bert-base-multilingual-uncased',\n",
    "    num_labels = 50,\n",
    "    output_attentions = False,\n",
    "    output_hidden_states = False,\n",
    ")\n",
    "\n",
    "# Recommended learning rates (Adam): 5e-5, 3e-5, 2e-5. See: https://arxiv.org/pdf/1810.04805.pdf\n",
    "optimizer = torch.optim.AdamW(model.parameters(), \n",
    "                              lr = 5e-5,\n",
    "                              eps = 1e-08\n",
    "                              )\n",
    "\n",
    "# Run on GPU\n",
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VA5Zgxk9pvLt",
    "outputId": "4d8ce539-7b3f-42bd-ec99-f8bc2d642b43"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:   0%|                                                                                                                                                                                                                      | 0/3 [00:00<?, ?it/s]<timed exec>:15: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13db8a1ae4b041b79bd2d99e06c17956",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6955 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Epoch:  33%|███████████████████████████████████████████████████████████████████▎                                                                                                                                      | 1/3 [56:38<1:53:16, 3398.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\t - Train loss: 1.6473\n",
      "\t - Validation Accuracy: 0.0146\n",
      "\t - Validation Precision: 0.4818\n",
      "\t - Validation Recall: 0.9161\n",
      "\t - Validation Specificity: 0.1950\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33a624cd7a7246c699d9782919e58b5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6955 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Epoch:  67%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋                                                                   | 2/3 [1:53:04<56:31, 3391.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\t - Train loss: 1.1347\n",
      "\t - Validation Accuracy: 0.0163\n",
      "\t - Validation Precision: 0.5223\n",
      "\t - Validation Recall: 0.7349\n",
      "\t - Validation Specificity: 0.2836\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "befaf111b16f42d9985bf192235ea7e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6955 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [2:49:30<00:00, 3390.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\t - Train loss: 0.8879\n",
      "\t - Validation Accuracy: 0.0166\n",
      "\t - Validation Precision: 0.6824\n",
      "\t - Validation Recall: 0.8962\n",
      "\t - Validation Specificity: 0.4529\n",
      "\n",
      "Wall time: 2h 49min 30s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Recommended number of epochs: 2, 3, 4. See: https://arxiv.org/pdf/1810.04805.pdf\n",
    "epochs = 3\n",
    "\n",
    "for _ in trange(epochs, desc = 'Epoch'):\n",
    "    \n",
    "    # ========== Training ==========\n",
    "    \n",
    "    # Set model to training mode\n",
    "    model.train()\n",
    "    \n",
    "    # Tracking variables\n",
    "    tr_loss = 0\n",
    "    nb_tr_examples, nb_tr_steps = 0, 0\n",
    "\n",
    "    for step, batch in enumerate(tqdm_notebook(train_dataloader)):\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "        optimizer.zero_grad()\n",
    "        # Forward pass\n",
    "        train_output = model(b_input_ids, \n",
    "                             token_type_ids = None, \n",
    "                             attention_mask = b_input_mask, \n",
    "                             labels = b_labels)\n",
    "        # Backward pass\n",
    "        train_output.loss.backward()\n",
    "        optimizer.step()\n",
    "        # Update tracking variables\n",
    "        tr_loss += train_output.loss.item()\n",
    "        nb_tr_examples += b_input_ids.size(0)\n",
    "        nb_tr_steps += 1\n",
    "\n",
    "    # ========== Validation ==========\n",
    "\n",
    "    # Set model to evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    # Tracking variables \n",
    "    val_accuracy = []\n",
    "    val_precision = []\n",
    "    val_recall = []\n",
    "    val_specificity = []\n",
    "\n",
    "    for batch in validation_dataloader:\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "        with torch.no_grad():\n",
    "          # Forward pass\n",
    "          eval_output = model(b_input_ids, \n",
    "                              token_type_ids = None, \n",
    "                              attention_mask = b_input_mask)\n",
    "        logits = eval_output.logits.detach().cpu().numpy()\n",
    "        label_ids = b_labels.to('cpu').numpy()\n",
    "        # Calculate validation metrics\n",
    "        b_accuracy, b_precision, b_recall, b_specificity = b_metrics(logits, label_ids)\n",
    "        val_accuracy.append(b_accuracy)\n",
    "        # Update precision only when (tp + fp) !=0; ignore nan\n",
    "        if b_precision != 'nan': val_precision.append(b_precision)\n",
    "        # Update recall only when (tp + fn) !=0; ignore nan\n",
    "        if b_recall != 'nan': val_recall.append(b_recall)\n",
    "        # Update specificity only when (tn + fp) !=0; ignore nan\n",
    "        if b_specificity != 'nan': val_specificity.append(b_specificity)\n",
    "\n",
    "    print('\\n\\t - Train loss: {:.4f}'.format(tr_loss / nb_tr_steps))\n",
    "    print('\\t - Validation Accuracy: {:.4f}'.format(sum(val_accuracy)/len(val_accuracy)))\n",
    "    print('\\t - Validation Precision: {:.4f}'.format(sum(val_precision)/len(val_precision)) if len(val_precision)>0 else '\\t - Validation Precision: NaN')\n",
    "    print('\\t - Validation Recall: {:.4f}'.format(sum(val_recall)/len(val_recall)) if len(val_recall)>0 else '\\t - Validation Recall: NaN')\n",
    "    print('\\t - Validation Specificity: {:.4f}\\n'.format(sum(val_specificity)/len(val_specificity)) if len(val_specificity)>0 else '\\t - Validation Specificity: NaN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "8r9ZN1_vpzdS"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Sentence:  a fantasy movie\n",
      "Predicted Class:  8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2364: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "def predict(sentence):\n",
    "    # We need Token IDs and Attention Mask for inference on the new sentence\n",
    "    test_ids = []\n",
    "    test_attention_mask = []\n",
    "\n",
    "    # Apply the tokenizer\n",
    "    encoding = preprocessing(new_sentence, tokenizer)\n",
    "\n",
    "    # Extract IDs and Attention Mask\n",
    "    test_ids.append(encoding['input_ids'])\n",
    "    test_attention_mask.append(encoding['attention_mask'])\n",
    "    test_ids = torch.cat(test_ids, dim = 0)\n",
    "    test_attention_mask = torch.cat(test_attention_mask, dim = 0)\n",
    "\n",
    "    # Forward pass, calculate logit predictions\n",
    "    with torch.no_grad():\n",
    "      output = model(test_ids.to(device), token_type_ids = None, attention_mask = test_attention_mask.to(device))\n",
    "\n",
    "    prediction = np.argmax(output.logits.cpu().numpy()).flatten().item()\n",
    "\n",
    "    print('Input Sentence: ', new_sentence)\n",
    "    print('Predicted Class: ', prediction)\n",
    "\n",
    "\n",
    "new_sentence = 'a fantasy movie'\n",
    "predict(new_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = './' \n",
    "torch.save(model, PATH + 'BERT_movie50_1.pt')  # 전체 모델 저장\n",
    "torch.save(model.state_dict(), PATH + 'model_state_dict50_1.pt')  # 모델 객체의 state_dict 저장"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bng0z-EfaGdr",
    "outputId": "ab5a4d3a-02fa-4887-d47e-b01a140a638b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\programdata\\anaconda3\\lib\\site-packages (4.29.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (2021.8.3)\n",
      "Requirement already satisfied: filelock in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (3.3.1)\n",
      "Requirement already satisfied: requests in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (2.26.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (0.13.3)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (21.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (1.20.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (4.62.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (0.14.1)\n",
      "Requirement already satisfied: fsspec in c:\\programdata\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2021.10.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\programdata\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (3.10.0.2)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from packaging>=20.0->transformers) (3.0.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\youngchan kim\\appdata\\roaming\\python\\python39\\site-packages (from tqdm>=4.27->transformers) (0.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->transformers) (3.2)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->transformers) (1.26.7)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->transformers) (2021.10.8)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -umpy (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -umpy (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -umpy (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -umpy (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -umpy (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -umpy (c:\\programdata\\anaconda3\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tabulate in c:\\programdata\\anaconda3\\lib\\site-packages (0.9.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -umpy (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -umpy (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -umpy (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -umpy (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -umpy (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -umpy (c:\\programdata\\anaconda3\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "!pip install tabulate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "8GDw68IjZzAW"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from tabulate import tabulate\n",
    "from tqdm import trange\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm, tqdm_notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 316
    },
    "id": "UglczWslaIK5",
    "outputId": "e2337623-2d02-4e71-d39f-3d1d22104e5b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label       int64\n",
      "url        object\n",
      "rating      int64\n",
      "date       object\n",
      "content    object\n",
      "dtype: object\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>url</th>\n",
       "      <th>rating</th>\n",
       "      <th>date</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>/review/rw1142613/</td>\n",
       "      <td>6</td>\n",
       "      <td>4 August 2005</td>\n",
       "      <td>It's an \"emotional manipulation for dummies\" t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>/review/rw1587268/</td>\n",
       "      <td>6</td>\n",
       "      <td>30 January 2007</td>\n",
       "      <td>Tim Robbins plays Andy Dufresne, a man convict...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>/review/rw0349436/</td>\n",
       "      <td>6</td>\n",
       "      <td>18 December 2003</td>\n",
       "      <td>It is the most basic of all principles of film...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>/review/rw0349359/</td>\n",
       "      <td>6</td>\n",
       "      <td>30 July 2003</td>\n",
       "      <td>Little more than average drama for popular tas...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>/review/rw0348197/</td>\n",
       "      <td>6</td>\n",
       "      <td>10 September 1998</td>\n",
       "      <td>An UNDER-RATED movie?? Gimme a break! This is ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                 url  rating               date  \\\n",
       "0      0  /review/rw1142613/       6      4 August 2005   \n",
       "1      0  /review/rw1587268/       6    30 January 2007   \n",
       "2      0  /review/rw0349436/       6   18 December 2003   \n",
       "3      0  /review/rw0349359/       6       30 July 2003   \n",
       "4      0  /review/rw0348197/       6  10 September 1998   \n",
       "\n",
       "                                             content  \n",
       "0  It's an \"emotional manipulation for dummies\" t...  \n",
       "1  Tim Robbins plays Andy Dufresne, a man convict...  \n",
       "2  It is the most basic of all principles of film...  \n",
       "3  Little more than average drama for popular tas...  \n",
       "4  An UNDER-RATED movie?? Gimme a break! This is ...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# file_path = './drive/MyDrive/movie_reviews.xlsx'\n",
    "file_path = './movie_reviews.xlsx'\n",
    "df = pd.read_excel(file_path)\n",
    "print(df.dtypes)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wc6gLWFChRDg",
    "outputId": "f25030a2-0e81-4151-aa4b-be5fcc115654"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\Youngchan\n",
      "[nltk_data]     Kim\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "5sL5ilC1mnFS"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1. 전체 문장'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''1. 전체 문장'''\n",
    "# text = df.content.values\n",
    "# labels = df.label.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "EbUlxK2VcRae"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 3.97 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "'''2. 두문장 씩 묶어서'''\n",
    "text = []\n",
    "labels = []\n",
    "content = df.content.values\n",
    "temp = df.label.values\n",
    "for i, sentence in enumerate(content):\n",
    "    if temp[i] >= 50: \n",
    "        break\n",
    "    try:\n",
    "        # print(content)\n",
    "        sentences = sent_tokenize(str(sentence))\n",
    "        for j in range(0, len(sentences)):\n",
    "            pair = sentences[j]+sentences[j + 1] if j + 1 < len(sentences) else sentences[j]\n",
    "            text.append(pair)\n",
    "            labels.append(temp[i])\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print(content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IrdAtjucoYqX"
   },
   "source": [
    "불용어 제거\n",
    "```\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize \n",
    "\n",
    "example = \"Family is not an important thing. It's everything.\"\n",
    "stop_words = set(stopwords.words('english')) \n",
    "\n",
    "word_tokens = word_tokenize(example)\n",
    "\n",
    "result = []\n",
    "for word in word_tokens: \n",
    "    if word not in stop_words: \n",
    "        result.append(word) \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SSbjA56cpT75"
   },
   "source": [
    "길이가 짧은 단어 삭제\n",
    "```\n",
    "import re\n",
    "text = \"I was wondering if anyone out there could enlighten me on this car.\"\n",
    "\n",
    "# 길이가 1~2인 단어들을 정규 표현식을 이용하여 삭제\n",
    "shortword = re.compile(r'\\W*\\b\\w{1,2}\\b')\n",
    "print(shortword.sub('', text))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6XkG3-vokj6H",
    "outputId": "2c496d2d-e796-4ccc-a3e6-9285e2baa7a7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "148367"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1H2ZSvQ7klr8",
    "outputId": "77272990-6ea2-46b8-8194-2cda94358bac"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "148367"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "DnFaJitvcbqj"
   },
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(\n",
    "    'bert-base-uncased',\n",
    "    do_lower_case = True # True/모두 소문자로 변환. False: 대소문자 구분\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "71g6AH6De0gn"
   },
   "source": [
    "bert-base-uncased  \n",
    "bert-base-cased  \n",
    "bert-base-multilingual-uncased  \n",
    "bert-base-multilingual-cased  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pW9ziJundlXT",
    "outputId": "156cd42d-f0f0-43b1-a666-27fb5f2c9317"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The script is well-written, and the cinematography and sound design are top-notch.This movie is a must-see for any war movie enthusiast, as well as anyone who appreciates masterful filmmaking.Rating: 9/10Quote: \"Earn this.\"- Captain John Miller (Tom Hanks)\n",
      "╒════════════════╤═════════════╕\n",
      "│ Tokens         │   Token IDs │\n",
      "╞════════════════╪═════════════╡\n",
      "│ the            │        1996 │\n",
      "├────────────────┼─────────────┤\n",
      "│ script         │        5896 │\n",
      "├────────────────┼─────────────┤\n",
      "│ is             │        2003 │\n",
      "├────────────────┼─────────────┤\n",
      "│ well           │        2092 │\n",
      "├────────────────┼─────────────┤\n",
      "│ -              │        1011 │\n",
      "├────────────────┼─────────────┤\n",
      "│ written        │        2517 │\n",
      "├────────────────┼─────────────┤\n",
      "│ ,              │        1010 │\n",
      "├────────────────┼─────────────┤\n",
      "│ and            │        1998 │\n",
      "├────────────────┼─────────────┤\n",
      "│ the            │        1996 │\n",
      "├────────────────┼─────────────┤\n",
      "│ cinematography │       16434 │\n",
      "├────────────────┼─────────────┤\n",
      "│ and            │        1998 │\n",
      "├────────────────┼─────────────┤\n",
      "│ sound          │        2614 │\n",
      "├────────────────┼─────────────┤\n",
      "│ design         │        2640 │\n",
      "├────────────────┼─────────────┤\n",
      "│ are            │        2024 │\n",
      "├────────────────┼─────────────┤\n",
      "│ top            │        2327 │\n",
      "├────────────────┼─────────────┤\n",
      "│ -              │        1011 │\n",
      "├────────────────┼─────────────┤\n",
      "│ notch          │       18624 │\n",
      "├────────────────┼─────────────┤\n",
      "│ .              │        1012 │\n",
      "├────────────────┼─────────────┤\n",
      "│ this           │        2023 │\n",
      "├────────────────┼─────────────┤\n",
      "│ movie          │        3185 │\n",
      "├────────────────┼─────────────┤\n",
      "│ is             │        2003 │\n",
      "├────────────────┼─────────────┤\n",
      "│ a              │        1037 │\n",
      "├────────────────┼─────────────┤\n",
      "│ must           │        2442 │\n",
      "├────────────────┼─────────────┤\n",
      "│ -              │        1011 │\n",
      "├────────────────┼─────────────┤\n",
      "│ see            │        2156 │\n",
      "├────────────────┼─────────────┤\n",
      "│ for            │        2005 │\n",
      "├────────────────┼─────────────┤\n",
      "│ any            │        2151 │\n",
      "├────────────────┼─────────────┤\n",
      "│ war            │        2162 │\n",
      "├────────────────┼─────────────┤\n",
      "│ movie          │        3185 │\n",
      "├────────────────┼─────────────┤\n",
      "│ enthusiast     │       29550 │\n",
      "├────────────────┼─────────────┤\n",
      "│ ,              │        1010 │\n",
      "├────────────────┼─────────────┤\n",
      "│ as             │        2004 │\n",
      "├────────────────┼─────────────┤\n",
      "│ well           │        2092 │\n",
      "├────────────────┼─────────────┤\n",
      "│ as             │        2004 │\n",
      "├────────────────┼─────────────┤\n",
      "│ anyone         │        3087 │\n",
      "├────────────────┼─────────────┤\n",
      "│ who            │        2040 │\n",
      "├────────────────┼─────────────┤\n",
      "│ appreciate     │        9120 │\n",
      "├────────────────┼─────────────┤\n",
      "│ ##s            │        2015 │\n",
      "├────────────────┼─────────────┤\n",
      "│ master         │        3040 │\n",
      "├────────────────┼─────────────┤\n",
      "│ ##ful          │        3993 │\n",
      "├────────────────┼─────────────┤\n",
      "│ filmmaking     │       24466 │\n",
      "├────────────────┼─────────────┤\n",
      "│ .              │        1012 │\n",
      "├────────────────┼─────────────┤\n",
      "│ rating         │        5790 │\n",
      "├────────────────┼─────────────┤\n",
      "│ :              │        1024 │\n",
      "├────────────────┼─────────────┤\n",
      "│ 9              │        1023 │\n",
      "├────────────────┼─────────────┤\n",
      "│ /              │        1013 │\n",
      "├────────────────┼─────────────┤\n",
      "│ 10             │        2184 │\n",
      "├────────────────┼─────────────┤\n",
      "│ ##qu           │       28940 │\n",
      "├────────────────┼─────────────┤\n",
      "│ ##ote          │       12184 │\n",
      "├────────────────┼─────────────┤\n",
      "│ :              │        1024 │\n",
      "├────────────────┼─────────────┤\n",
      "│ \"              │        1000 │\n",
      "├────────────────┼─────────────┤\n",
      "│ earn           │        7796 │\n",
      "├────────────────┼─────────────┤\n",
      "│ this           │        2023 │\n",
      "├────────────────┼─────────────┤\n",
      "│ .              │        1012 │\n",
      "├────────────────┼─────────────┤\n",
      "│ \"              │        1000 │\n",
      "├────────────────┼─────────────┤\n",
      "│ -              │        1011 │\n",
      "├────────────────┼─────────────┤\n",
      "│ captain        │        2952 │\n",
      "├────────────────┼─────────────┤\n",
      "│ john           │        2198 │\n",
      "├────────────────┼─────────────┤\n",
      "│ miller         │        4679 │\n",
      "├────────────────┼─────────────┤\n",
      "│ (              │        1006 │\n",
      "├────────────────┼─────────────┤\n",
      "│ tom            │        3419 │\n",
      "├────────────────┼─────────────┤\n",
      "│ hank           │        9180 │\n",
      "├────────────────┼─────────────┤\n",
      "│ ##s            │        2015 │\n",
      "├────────────────┼─────────────┤\n",
      "│ )              │        1007 │\n",
      "╘════════════════╧═════════════╛\n"
     ]
    }
   ],
   "source": [
    "def print_rand_sentence():\n",
    "    '''Displays the tokens and respective IDs of a random text sample'''\n",
    "    index = random.randint(0, len(text)-1)\n",
    "    print(text[index])\n",
    "    table = np.array([tokenizer.tokenize(text[index]),\n",
    "                      tokenizer.convert_tokens_to_ids(tokenizer.tokenize(text[index]))]).T\n",
    "    print(tabulate(table, headers = ['Tokens', 'Token IDs'], tablefmt = 'fancy_grid'))\n",
    "\n",
    "print_rand_sentence()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "j7tJh_fNlk3v",
    "outputId": "1d53872f-fd15-4fa6-9e82-520bddd9d0e7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It's an \"emotional manipulation for dummies\" type of film, with boring direction and cardboard characters.Nice performances by the actors though, some great moments, but nothing outside the \"Hallmark meets Stephen King meets Prison\" feel of the entire movie.I saw it one and a half times, and would see it again only if I got paid for the trouble.Similarly, I hated 'The Green Mile\", only later did I find out it's the work of Darabont.My recommendation: Skip this, and go see OZ again.\n",
      "╒════════════════╤═════════════╕\n",
      "│ Tokens         │   Token IDs │\n",
      "╞════════════════╪═════════════╡\n",
      "│ it             │        2009 │\n",
      "├────────────────┼─────────────┤\n",
      "│ '              │        1005 │\n",
      "├────────────────┼─────────────┤\n",
      "│ s              │        1055 │\n",
      "├────────────────┼─────────────┤\n",
      "│ an             │        2019 │\n",
      "├────────────────┼─────────────┤\n",
      "│ \"              │        1000 │\n",
      "├────────────────┼─────────────┤\n",
      "│ emotional      │        6832 │\n",
      "├────────────────┼─────────────┤\n",
      "│ manipulation   │       16924 │\n",
      "├────────────────┼─────────────┤\n",
      "│ for            │        2005 │\n",
      "├────────────────┼─────────────┤\n",
      "│ du             │        4241 │\n",
      "├────────────────┼─────────────┤\n",
      "│ ##mm           │        7382 │\n",
      "├────────────────┼─────────────┤\n",
      "│ ##ies          │        3111 │\n",
      "├────────────────┼─────────────┤\n",
      "│ \"              │        1000 │\n",
      "├────────────────┼─────────────┤\n",
      "│ type           │        2828 │\n",
      "├────────────────┼─────────────┤\n",
      "│ of             │        1997 │\n",
      "├────────────────┼─────────────┤\n",
      "│ film           │        2143 │\n",
      "├────────────────┼─────────────┤\n",
      "│ ,              │        1010 │\n",
      "├────────────────┼─────────────┤\n",
      "│ with           │        2007 │\n",
      "├────────────────┼─────────────┤\n",
      "│ boring         │       11771 │\n",
      "├────────────────┼─────────────┤\n",
      "│ direction      │        3257 │\n",
      "├────────────────┼─────────────┤\n",
      "│ and            │        1998 │\n",
      "├────────────────┼─────────────┤\n",
      "│ cardboard      │       19747 │\n",
      "├────────────────┼─────────────┤\n",
      "│ characters     │        3494 │\n",
      "├────────────────┼─────────────┤\n",
      "│ .              │        1012 │\n",
      "├────────────────┼─────────────┤\n",
      "│ nice           │        3835 │\n",
      "├────────────────┼─────────────┤\n",
      "│ performances   │        4616 │\n",
      "├────────────────┼─────────────┤\n",
      "│ by             │        2011 │\n",
      "├────────────────┼─────────────┤\n",
      "│ the            │        1996 │\n",
      "├────────────────┼─────────────┤\n",
      "│ actors         │        5889 │\n",
      "├────────────────┼─────────────┤\n",
      "│ though         │        2295 │\n",
      "├────────────────┼─────────────┤\n",
      "│ ,              │        1010 │\n",
      "├────────────────┼─────────────┤\n",
      "│ some           │        2070 │\n",
      "├────────────────┼─────────────┤\n",
      "│ great          │        2307 │\n",
      "├────────────────┼─────────────┤\n",
      "│ moments        │        5312 │\n",
      "├────────────────┼─────────────┤\n",
      "│ ,              │        1010 │\n",
      "├────────────────┼─────────────┤\n",
      "│ but            │        2021 │\n",
      "├────────────────┼─────────────┤\n",
      "│ nothing        │        2498 │\n",
      "├────────────────┼─────────────┤\n",
      "│ outside        │        2648 │\n",
      "├────────────────┼─────────────┤\n",
      "│ the            │        1996 │\n",
      "├────────────────┼─────────────┤\n",
      "│ \"              │        1000 │\n",
      "├────────────────┼─────────────┤\n",
      "│ hallmark       │       25812 │\n",
      "├────────────────┼─────────────┤\n",
      "│ meets          │        6010 │\n",
      "├────────────────┼─────────────┤\n",
      "│ stephen        │        4459 │\n",
      "├────────────────┼─────────────┤\n",
      "│ king           │        2332 │\n",
      "├────────────────┼─────────────┤\n",
      "│ meets          │        6010 │\n",
      "├────────────────┼─────────────┤\n",
      "│ prison         │        3827 │\n",
      "├────────────────┼─────────────┤\n",
      "│ \"              │        1000 │\n",
      "├────────────────┼─────────────┤\n",
      "│ feel           │        2514 │\n",
      "├────────────────┼─────────────┤\n",
      "│ of             │        1997 │\n",
      "├────────────────┼─────────────┤\n",
      "│ the            │        1996 │\n",
      "├────────────────┼─────────────┤\n",
      "│ entire         │        2972 │\n",
      "├────────────────┼─────────────┤\n",
      "│ movie          │        3185 │\n",
      "├────────────────┼─────────────┤\n",
      "│ .              │        1012 │\n",
      "├────────────────┼─────────────┤\n",
      "│ i              │        1045 │\n",
      "├────────────────┼─────────────┤\n",
      "│ saw            │        2387 │\n",
      "├────────────────┼─────────────┤\n",
      "│ it             │        2009 │\n",
      "├────────────────┼─────────────┤\n",
      "│ one            │        2028 │\n",
      "├────────────────┼─────────────┤\n",
      "│ and            │        1998 │\n",
      "├────────────────┼─────────────┤\n",
      "│ a              │        1037 │\n",
      "├────────────────┼─────────────┤\n",
      "│ half           │        2431 │\n",
      "├────────────────┼─────────────┤\n",
      "│ times          │        2335 │\n",
      "├────────────────┼─────────────┤\n",
      "│ ,              │        1010 │\n",
      "├────────────────┼─────────────┤\n",
      "│ and            │        1998 │\n",
      "├────────────────┼─────────────┤\n",
      "│ would          │        2052 │\n",
      "├────────────────┼─────────────┤\n",
      "│ see            │        2156 │\n",
      "├────────────────┼─────────────┤\n",
      "│ it             │        2009 │\n",
      "├────────────────┼─────────────┤\n",
      "│ again          │        2153 │\n",
      "├────────────────┼─────────────┤\n",
      "│ only           │        2069 │\n",
      "├────────────────┼─────────────┤\n",
      "│ if             │        2065 │\n",
      "├────────────────┼─────────────┤\n",
      "│ i              │        1045 │\n",
      "├────────────────┼─────────────┤\n",
      "│ got            │        2288 │\n",
      "├────────────────┼─────────────┤\n",
      "│ paid           │        3825 │\n",
      "├────────────────┼─────────────┤\n",
      "│ for            │        2005 │\n",
      "├────────────────┼─────────────┤\n",
      "│ the            │        1996 │\n",
      "├────────────────┼─────────────┤\n",
      "│ trouble        │        4390 │\n",
      "├────────────────┼─────────────┤\n",
      "│ .              │        1012 │\n",
      "├────────────────┼─────────────┤\n",
      "│ similarly      │        6660 │\n",
      "├────────────────┼─────────────┤\n",
      "│ ,              │        1010 │\n",
      "├────────────────┼─────────────┤\n",
      "│ i              │        1045 │\n",
      "├────────────────┼─────────────┤\n",
      "│ hated          │        6283 │\n",
      "├────────────────┼─────────────┤\n",
      "│ '              │        1005 │\n",
      "├────────────────┼─────────────┤\n",
      "│ the            │        1996 │\n",
      "├────────────────┼─────────────┤\n",
      "│ green          │        2665 │\n",
      "├────────────────┼─────────────┤\n",
      "│ mile           │        3542 │\n",
      "├────────────────┼─────────────┤\n",
      "│ \"              │        1000 │\n",
      "├────────────────┼─────────────┤\n",
      "│ ,              │        1010 │\n",
      "├────────────────┼─────────────┤\n",
      "│ only           │        2069 │\n",
      "├────────────────┼─────────────┤\n",
      "│ later          │        2101 │\n",
      "├────────────────┼─────────────┤\n",
      "│ did            │        2106 │\n",
      "├────────────────┼─────────────┤\n",
      "│ i              │        1045 │\n",
      "├────────────────┼─────────────┤\n",
      "│ find           │        2424 │\n",
      "├────────────────┼─────────────┤\n",
      "│ out            │        2041 │\n",
      "├────────────────┼─────────────┤\n",
      "│ it             │        2009 │\n",
      "├────────────────┼─────────────┤\n",
      "│ '              │        1005 │\n",
      "├────────────────┼─────────────┤\n",
      "│ s              │        1055 │\n",
      "├────────────────┼─────────────┤\n",
      "│ the            │        1996 │\n",
      "├────────────────┼─────────────┤\n",
      "│ work           │        2147 │\n",
      "├────────────────┼─────────────┤\n",
      "│ of             │        1997 │\n",
      "├────────────────┼─────────────┤\n",
      "│ dar            │       18243 │\n",
      "├────────────────┼─────────────┤\n",
      "│ ##ab           │        7875 │\n",
      "├────────────────┼─────────────┤\n",
      "│ ##ont          │       12162 │\n",
      "├────────────────┼─────────────┤\n",
      "│ .              │        1012 │\n",
      "├────────────────┼─────────────┤\n",
      "│ my             │        2026 │\n",
      "├────────────────┼─────────────┤\n",
      "│ recommendation │       12832 │\n",
      "├────────────────┼─────────────┤\n",
      "│ :              │        1024 │\n",
      "├────────────────┼─────────────┤\n",
      "│ skip           │       13558 │\n",
      "├────────────────┼─────────────┤\n",
      "│ this           │        2023 │\n",
      "├────────────────┼─────────────┤\n",
      "│ ,              │        1010 │\n",
      "├────────────────┼─────────────┤\n",
      "│ and            │        1998 │\n",
      "├────────────────┼─────────────┤\n",
      "│ go             │        2175 │\n",
      "├────────────────┼─────────────┤\n",
      "│ see            │        2156 │\n",
      "├────────────────┼─────────────┤\n",
      "│ oz             │       11472 │\n",
      "├────────────────┼─────────────┤\n",
      "│ again          │        2153 │\n",
      "├────────────────┼─────────────┤\n",
      "│ .              │        1012 │\n",
      "╘════════════════╧═════════════╛\n"
     ]
    }
   ],
   "source": [
    "def print_sentence(index):\n",
    "    print(text[index])\n",
    "    table = np.array([tokenizer.tokenize(text[index]),\n",
    "                      tokenizer.convert_tokens_to_ids(tokenizer.tokenize(text[index]))]).T\n",
    "    print(tabulate(table, headers = ['Tokens', 'Token IDs'], tablefmt = 'fancy_grid'))\n",
    "\n",
    "print_sentence(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NAn7gRGVeNR5",
    "outputId": "439bdd1c-eec1-40c9-c0f5-887dc929d1e0"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2364: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 2min 23s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "token_id = []\n",
    "attention_masks = []\n",
    "\n",
    "def preprocessing(input_text, tokenizer):\n",
    "  '''\n",
    "  Returns <class transformers.tokenization_utils_base.BatchEncoding> with the following fields:\n",
    "    - input_ids: list of token ids\n",
    "    - token_type_ids: list of token type ids\n",
    "    - attention_mask: list of indices (0,1) specifying which tokens should considered by the model (return_attention_mask = True).\n",
    "  '''\n",
    "  return tokenizer.encode_plus(\n",
    "                        input_text,\n",
    "                        add_special_tokens = True,\n",
    "                        max_length = 128,\n",
    "                        pad_to_max_length = True,\n",
    "                        return_attention_mask = True,\n",
    "                        return_tensors = 'pt'\n",
    "                   )\n",
    "\n",
    "\n",
    "for sample in text:\n",
    "    encoding_dict = preprocessing(sample, tokenizer)\n",
    "    token_id.append(encoding_dict['input_ids']) \n",
    "    attention_masks.append(encoding_dict['attention_mask'])\n",
    "\n",
    "token_id = torch.cat(token_id, dim = 0)\n",
    "attention_masks = torch.cat(attention_masks, dim = 0)\n",
    "labels = torch.tensor(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AnF5OF0PfaiB",
    "outputId": "583ca210-b47d-427e-ec1e-1387e344a3f9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  101,  3835,  4616,  2011,  1996,  5889,  2295,  1010,  2070,  2307,\n",
       "         5312,  1010,  2021,  2498,  2648,  1996,  1000, 25812,  6010,  4459,\n",
       "         2332,  6010,  3827,  1000,  2514,  1997,  1996,  2972,  3185,  1012,\n",
       "         1045,  2387,  2009,  2028,  1998,  1037,  2431,  2335,  1010,  1998,\n",
       "         2052,  2156,  2009,  2153,  2069,  2065,  1045,  2288,  3825,  2005,\n",
       "         1996,  4390,  1012,  6660,  1010,  1045,  6283,  1005,  1996,  2665,\n",
       "         3542,  1000,  1010,  2069,  2101,  2106,  1045,  2424,  2041,  2009,\n",
       "         1005,  1055,  1996,  2147,  1997, 18243,  7875, 12162,  1012,  2026,\n",
       "        12832,  1024, 13558,  2023,  1010,  1998,  2175,  2156, 11472,  2153,\n",
       "         1012,   102,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_id[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jASZmzkfnD-7",
    "outputId": "7623dc5d-f79f-45a5-f5e8-bd59007eb693"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "╒═══════════════╤═════════════╤══════════════════╕\n",
      "│ Tokens        │   Token IDs │   Attention Mask │\n",
      "╞═══════════════╪═════════════╪══════════════════╡\n",
      "│ [CLS]         │         101 │                1 │\n",
      "├───────────────┼─────────────┼──────────────────┤\n",
      "│ it            │        2009 │                1 │\n",
      "├───────────────┼─────────────┼──────────────────┤\n",
      "│ '             │        1005 │                1 │\n",
      "├───────────────┼─────────────┼──────────────────┤\n",
      "│ s             │        1055 │                1 │\n",
      "├───────────────┼─────────────┼──────────────────┤\n",
      "│ not           │        2025 │                1 │\n",
      "├───────────────┼─────────────┼──────────────────┤\n",
      "│ entertainment │        4024 │                1 │\n",
      "├───────────────┼─────────────┼──────────────────┤\n",
      "│ ,             │        1010 │                1 │\n",
      "├───────────────┼─────────────┼──────────────────┤\n",
      "│ it            │        2009 │                1 │\n",
      "├───────────────┼─────────────┼──────────────────┤\n",
      "│ '             │        1005 │                1 │\n",
      "├───────────────┼─────────────┼──────────────────┤\n",
      "│ s             │        1055 │                1 │\n",
      "├───────────────┼─────────────┼──────────────────┤\n",
      "│ de            │        2139 │                1 │\n",
      "├───────────────┼─────────────┼──────────────────┤\n",
      "│ ##pressing    │       24128 │                1 │\n",
      "├───────────────┼─────────────┼──────────────────┤\n",
      "│ .             │        1012 │                1 │\n",
      "├───────────────┼─────────────┼──────────────────┤\n",
      "│ the           │        1996 │                1 │\n",
      "├───────────────┼─────────────┼──────────────────┤\n",
      "│ lead          │        2599 │                1 │\n",
      "├───────────────┼─────────────┼──────────────────┤\n",
      "│ actor         │        3364 │                1 │\n",
      "├───────────────┼─────────────┼──────────────────┤\n",
      "│ is            │        2003 │                1 │\n",
      "├───────────────┼─────────────┼──────────────────┤\n",
      "│ excellent     │        6581 │                1 │\n",
      "├───────────────┼─────────────┼──────────────────┤\n",
      "│ .             │        1012 │                1 │\n",
      "├───────────────┼─────────────┼──────────────────┤\n",
      "│ [SEP]         │         102 │                1 │\n",
      "├───────────────┼─────────────┼──────────────────┤\n",
      "│ [PAD]         │           0 │                0 │\n",
      "├───────────────┼─────────────┼──────────────────┤\n",
      "│ [PAD]         │           0 │                0 │\n",
      "├───────────────┼─────────────┼──────────────────┤\n",
      "│ [PAD]         │           0 │                0 │\n",
      "├───────────────┼─────────────┼──────────────────┤\n",
      "│ [PAD]         │           0 │                0 │\n",
      "├───────────────┼─────────────┼──────────────────┤\n",
      "│ [PAD]         │           0 │                0 │\n",
      "├───────────────┼─────────────┼──────────────────┤\n",
      "│ [PAD]         │           0 │                0 │\n",
      "├───────────────┼─────────────┼──────────────────┤\n",
      "│ [PAD]         │           0 │                0 │\n",
      "├───────────────┼─────────────┼──────────────────┤\n",
      "│ [PAD]         │           0 │                0 │\n",
      "├───────────────┼─────────────┼──────────────────┤\n",
      "│ [PAD]         │           0 │                0 │\n",
      "├───────────────┼─────────────┼──────────────────┤\n",
      "│ [PAD]         │           0 │                0 │\n",
      "├───────────────┼─────────────┼──────────────────┤\n",
      "│ [PAD]         │           0 │                0 │\n",
      "├───────────────┼─────────────┼──────────────────┤\n",
      "│ [PAD]         │           0 │                0 │\n",
      "├───────────────┼─────────────┼──────────────────┤\n",
      "│ [PAD]         │           0 │                0 │\n",
      "├───────────────┼─────────────┼──────────────────┤\n",
      "│ [PAD]         │           0 │                0 │\n",
      "├───────────────┼─────────────┼──────────────────┤\n",
      "│ [PAD]         │           0 │                0 │\n",
      "├───────────────┼─────────────┼──────────────────┤\n",
      "│ [PAD]         │           0 │                0 │\n",
      "├───────────────┼─────────────┼──────────────────┤\n",
      "│ [PAD]         │           0 │                0 │\n",
      "├───────────────┼─────────────┼──────────────────┤\n",
      "│ [PAD]         │           0 │                0 │\n",
      "├───────────────┼─────────────┼──────────────────┤\n",
      "│ [PAD]         │           0 │                0 │\n",
      "├───────────────┼─────────────┼──────────────────┤\n",
      "│ [PAD]         │           0 │                0 │\n",
      "├───────────────┼─────────────┼──────────────────┤\n",
      "│ [PAD]         │           0 │                0 │\n",
      "├───────────────┼─────────────┼──────────────────┤\n",
      "│ [PAD]         │           0 │                0 │\n",
      "├───────────────┼─────────────┼──────────────────┤\n",
      "│ [PAD]         │           0 │                0 │\n",
      "├───────────────┼─────────────┼──────────────────┤\n",
      "│ [PAD]         │           0 │                0 │\n",
      "├───────────────┼─────────────┼──────────────────┤\n",
      "│ [PAD]         │           0 │                0 │\n",
      "├───────────────┼─────────────┼──────────────────┤\n",
      "│ [PAD]         │           0 │                0 │\n",
      "├───────────────┼─────────────┼──────────────────┤\n",
      "│ [PAD]         │           0 │                0 │\n",
      "├───────────────┼─────────────┼──────────────────┤\n",
      "│ [PAD]         │           0 │                0 │\n",
      "├───────────────┼─────────────┼──────────────────┤\n",
      "│ [PAD]         │           0 │                0 │\n",
      "├───────────────┼─────────────┼──────────────────┤\n",
      "│ [PAD]         │           0 │                0 │\n",
      "├───────────────┼─────────────┼──────────────────┤\n",
      "│ [PAD]         │           0 │                0 │\n",
      "├───────────────┼─────────────┼──────────────────┤\n",
      "│ [PAD]         │           0 │                0 │\n",
      "├───────────────┼─────────────┼──────────────────┤\n",
      "│ [PAD]         │           0 │                0 │\n",
      "├───────────────┼─────────────┼──────────────────┤\n",
      "│ [PAD]         │           0 │                0 │\n",
      "├───────────────┼─────────────┼──────────────────┤\n",
      "│ [PAD]         │           0 │                0 │\n",
      "├───────────────┼─────────────┼──────────────────┤\n",
      "│ [PAD]         │           0 │                0 │\n",
      "├───────────────┼─────────────┼──────────────────┤\n",
      "│ [PAD]         │           0 │                0 │\n",
      "├───────────────┼─────────────┼──────────────────┤\n",
      "│ [PAD]         │           0 │                0 │\n",
      "├───────────────┼─────────────┼──────────────────┤\n",
      "│ [PAD]         │           0 │                0 │\n",
      "├───────────────┼─────────────┼──────────────────┤\n",
      "│ [PAD]         │           0 │                0 │\n",
      "├───────────────┼─────────────┼──────────────────┤\n",
      "│ [PAD]         │           0 │                0 │\n",
      "├───────────────┼─────────────┼──────────────────┤\n",
      "│ [PAD]         │           0 │                0 │\n",
      "├───────────────┼─────────────┼──────────────────┤\n",
      "│ [PAD]         │           0 │                0 │\n",
      "├───────────────┼─────────────┼──────────────────┤\n",
      "│ [PAD]         │           0 │                0 │\n",
      "├───────────────┼─────────────┼──────────────────┤\n",
      "│ [PAD]         │           0 │                0 │\n",
      "├───────────────┼─────────────┼──────────────────┤\n",
      "│ [PAD]         │           0 │                0 │\n",
      "├───────────────┼─────────────┼──────────────────┤\n",
      "│ [PAD]         │           0 │                0 │\n",
      "├───────────────┼─────────────┼──────────────────┤\n",
      "│ [PAD]         │           0 │                0 │\n",
      "├───────────────┼─────────────┼──────────────────┤\n",
      "│ [PAD]         │           0 │                0 │\n",
      "├───────────────┼─────────────┼──────────────────┤\n",
      "│ [PAD]         │           0 │                0 │\n",
      "├───────────────┼─────────────┼──────────────────┤\n",
      "│ [PAD]         │           0 │                0 │\n",
      "├───────────────┼─────────────┼──────────────────┤\n",
      "│ [PAD]         │           0 │                0 │\n",
      "├───────────────┼─────────────┼──────────────────┤\n",
      "│ [PAD]         │           0 │                0 │\n",
      "├───────────────┼─────────────┼──────────────────┤\n",
      "│ [PAD]         │           0 │                0 │\n",
      "├───────────────┼─────────────┼──────────────────┤\n",
      "│ [PAD]         │           0 │                0 │\n",
      "├───────────────┼─────────────┼──────────────────┤\n",
      "│ [PAD]         │           0 │                0 │\n",
      "├───────────────┼─────────────┼──────────────────┤\n",
      "│ [PAD]         │           0 │                0 │\n",
      "├───────────────┼─────────────┼──────────────────┤\n",
      "│ [PAD]         │           0 │                0 │\n",
      "├───────────────┼─────────────┼──────────────────┤\n",
      "│ [PAD]         │           0 │                0 │\n",
      "├───────────────┼─────────────┼──────────────────┤\n",
      "│ [PAD]         │           0 │                0 │\n",
      "├───────────────┼─────────────┼──────────────────┤\n",
      "│ [PAD]         │           0 │                0 │\n",
      "├───────────────┼─────────────┼──────────────────┤\n",
      "│ [PAD]         │           0 │                0 │\n",
      "├───────────────┼─────────────┼──────────────────┤\n",
      "│ [PAD]         │           0 │                0 │\n",
      "├───────────────┼─────────────┼──────────────────┤\n",
      "│ [PAD]         │           0 │                0 │\n",
      "├───────────────┼─────────────┼──────────────────┤\n",
      "│ [PAD]         │           0 │                0 │\n",
      "├───────────────┼─────────────┼──────────────────┤\n",
      "│ [PAD]         │           0 │                0 │\n",
      "├───────────────┼─────────────┼──────────────────┤\n",
      "│ [PAD]         │           0 │                0 │\n",
      "├───────────────┼─────────────┼──────────────────┤\n",
      "│ [PAD]         │           0 │                0 │\n",
      "├───────────────┼─────────────┼──────────────────┤\n",
      "│ [PAD]         │           0 │                0 │\n",
      "├───────────────┼─────────────┼──────────────────┤\n",
      "│ [PAD]         │           0 │                0 │\n",
      "├───────────────┼─────────────┼──────────────────┤\n",
      "│ [PAD]         │           0 │                0 │\n",
      "├───────────────┼─────────────┼──────────────────┤\n",
      "│ [PAD]         │           0 │                0 │\n",
      "├───────────────┼─────────────┼──────────────────┤\n",
      "│ [PAD]         │           0 │                0 │\n",
      "├───────────────┼─────────────┼──────────────────┤\n",
      "│ [PAD]         │           0 │                0 │\n",
      "├───────────────┼─────────────┼──────────────────┤\n",
      "│ [PAD]         │           0 │                0 │\n",
      "├───────────────┼─────────────┼──────────────────┤\n",
      "│ [PAD]         │           0 │                0 │\n",
      "├───────────────┼─────────────┼──────────────────┤\n",
      "│ [PAD]         │           0 │                0 │\n",
      "├───────────────┼─────────────┼──────────────────┤\n",
      "│ [PAD]         │           0 │                0 │\n",
      "├───────────────┼─────────────┼──────────────────┤\n",
      "│ [PAD]         │           0 │                0 │\n",
      "├───────────────┼─────────────┼──────────────────┤\n",
      "│ [PAD]         │           0 │                0 │\n",
      "├───────────────┼─────────────┼──────────────────┤\n",
      "│ [PAD]         │           0 │                0 │\n",
      "├───────────────┼─────────────┼──────────────────┤\n",
      "│ [PAD]         │           0 │                0 │\n",
      "├───────────────┼─────────────┼──────────────────┤\n",
      "│ [PAD]         │           0 │                0 │\n",
      "├───────────────┼─────────────┼──────────────────┤\n",
      "│ [PAD]         │           0 │                0 │\n",
      "├───────────────┼─────────────┼──────────────────┤\n",
      "│ [PAD]         │           0 │                0 │\n",
      "├───────────────┼─────────────┼──────────────────┤\n",
      "│ [PAD]         │           0 │                0 │\n",
      "├───────────────┼─────────────┼──────────────────┤\n",
      "│ [PAD]         │           0 │                0 │\n",
      "├───────────────┼─────────────┼──────────────────┤\n",
      "│ [PAD]         │           0 │                0 │\n",
      "├───────────────┼─────────────┼──────────────────┤\n",
      "│ [PAD]         │           0 │                0 │\n",
      "├───────────────┼─────────────┼──────────────────┤\n",
      "│ [PAD]         │           0 │                0 │\n",
      "├───────────────┼─────────────┼──────────────────┤\n",
      "│ [PAD]         │           0 │                0 │\n",
      "├───────────────┼─────────────┼──────────────────┤\n",
      "│ [PAD]         │           0 │                0 │\n",
      "├───────────────┼─────────────┼──────────────────┤\n",
      "│ [PAD]         │           0 │                0 │\n",
      "├───────────────┼─────────────┼──────────────────┤\n",
      "│ [PAD]         │           0 │                0 │\n",
      "├───────────────┼─────────────┼──────────────────┤\n",
      "│ [PAD]         │           0 │                0 │\n",
      "├───────────────┼─────────────┼──────────────────┤\n",
      "│ [PAD]         │           0 │                0 │\n",
      "├───────────────┼─────────────┼──────────────────┤\n",
      "│ [PAD]         │           0 │                0 │\n",
      "├───────────────┼─────────────┼──────────────────┤\n",
      "│ [PAD]         │           0 │                0 │\n",
      "├───────────────┼─────────────┼──────────────────┤\n",
      "│ [PAD]         │           0 │                0 │\n",
      "├───────────────┼─────────────┼──────────────────┤\n",
      "│ [PAD]         │           0 │                0 │\n",
      "├───────────────┼─────────────┼──────────────────┤\n",
      "│ [PAD]         │           0 │                0 │\n",
      "├───────────────┼─────────────┼──────────────────┤\n",
      "│ [PAD]         │           0 │                0 │\n",
      "├───────────────┼─────────────┼──────────────────┤\n",
      "│ [PAD]         │           0 │                0 │\n",
      "├───────────────┼─────────────┼──────────────────┤\n",
      "│ [PAD]         │           0 │                0 │\n",
      "├───────────────┼─────────────┼──────────────────┤\n",
      "│ [PAD]         │           0 │                0 │\n",
      "├───────────────┼─────────────┼──────────────────┤\n",
      "│ [PAD]         │           0 │                0 │\n",
      "├───────────────┼─────────────┼──────────────────┤\n",
      "│ [PAD]         │           0 │                0 │\n",
      "├───────────────┼─────────────┼──────────────────┤\n",
      "│ [PAD]         │           0 │                0 │\n",
      "╘═══════════════╧═════════════╧══════════════════╛\n",
      "Wall time: 7.98 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def print_rand_sentence_encoding():\n",
    "  '''Displays tokens, token IDs and attention mask of a random text sample'''\n",
    "  index = random.randint(0, len(text) - 1)\n",
    "  tokens = tokenizer.tokenize(tokenizer.decode(token_id[index]))\n",
    "  token_ids = [i.numpy() for i in token_id[index]]\n",
    "  attention = [i.numpy() for i in attention_masks[index]]\n",
    "\n",
    "  table = np.array([tokens, token_ids, attention]).T\n",
    "  print(tabulate(table, headers = ['Tokens', 'Token IDs', 'Attention Mask'], tablefmt = 'fancy_grid'))\n",
    "\n",
    "print_rand_sentence_encoding()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "eGBNmXMZnGya"
   },
   "outputs": [],
   "source": [
    "val_ratio = 0.25\n",
    "batch_size = 16\n",
    "\n",
    "# Indices of the train and validation splits stratified by labels\n",
    "train_idx, val_idx = train_test_split(\n",
    "    np.arange(len(labels)),\n",
    "    test_size = val_ratio,\n",
    "    shuffle = True,\n",
    "    stratify = labels)\n",
    "\n",
    "# Train and validation sets\n",
    "train_set = TensorDataset(token_id[train_idx], \n",
    "                          attention_masks[train_idx], \n",
    "                          labels[train_idx])\n",
    "\n",
    "val_set = TensorDataset(token_id[val_idx], \n",
    "                        attention_masks[val_idx], \n",
    "                        labels[val_idx])\n",
    "\n",
    "# Prepare DataLoader\n",
    "train_dataloader = DataLoader(\n",
    "            train_set,\n",
    "            sampler = RandomSampler(train_set),\n",
    "            batch_size = batch_size\n",
    "        )\n",
    "\n",
    "validation_dataloader = DataLoader(\n",
    "            val_set,\n",
    "            sampler = SequentialSampler(val_set),\n",
    "            batch_size = batch_size\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "ozF5dCmUphm_"
   },
   "outputs": [],
   "source": [
    "def b_tp(preds, labels):\n",
    "    '''Returns True Positives (TP): count of correct predictions of actual class 1'''\n",
    "    return sum([preds == labels and preds == 1 for preds, labels in zip(preds, labels)])\n",
    "\n",
    "def b_fp(preds, labels):\n",
    "    '''Returns False Positives (FP): count of wrong predictions of actual class 1'''\n",
    "    return sum([preds != labels and preds == 1 for preds, labels in zip(preds, labels)])\n",
    "\n",
    "def b_tn(preds, labels):\n",
    "    '''Returns True Negatives (TN): count of correct predictions of actual class 0'''\n",
    "    return sum([preds == labels and preds == 0 for preds, labels in zip(preds, labels)])\n",
    "\n",
    "def b_fn(preds, labels):\n",
    "    '''Returns False Negatives (FN): count of wrong predictions of actual class 0'''\n",
    "    return sum([preds != labels and preds == 0 for preds, labels in zip(preds, labels)])\n",
    "\n",
    "def b_metrics(preds, labels):\n",
    "    '''\n",
    "    Returns the following metrics:\n",
    "        - accuracy    = (TP + TN) / N\n",
    "        - precision   = TP / (TP + FP)\n",
    "        - recall      = TP / (TP + FN)\n",
    "        - specificity = TN / (TN + FP)\n",
    "    '''\n",
    "    preds = np.argmax(preds, axis = 1).flatten()\n",
    "    labels = labels.flatten()\n",
    "    tp = b_tp(preds, labels)\n",
    "    tn = b_tn(preds, labels)\n",
    "    fp = b_fp(preds, labels)\n",
    "    fn = b_fn(preds, labels)\n",
    "    b_accuracy = (tp + tn) / len(labels)\n",
    "    b_precision = tp / (tp + fp) if (tp + fp) > 0 else 'nan'\n",
    "    b_recall = tp / (tp + fn) if (tp + fn) > 0 else 'nan'\n",
    "    b_specificity = tn / (tn + fp) if (tn + fp) > 0 else 'nan'\n",
    "    return b_accuracy, b_precision, b_recall, b_specificity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2vRSUOt4pnDh",
    "outputId": "51aca52c-ad55-43ba-ee7c-2e371d3920b5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1.41 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=50, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    'bert-base-uncased',\n",
    "    num_labels = 50,\n",
    "    output_attentions = False,\n",
    "    output_hidden_states = False,\n",
    ")\n",
    "\n",
    "# Recommended learning rates (Adam): 5e-5, 3e-5, 2e-5. See: https://arxiv.org/pdf/1810.04805.pdf\n",
    "optimizer = torch.optim.AdamW(model.parameters(), \n",
    "                              lr = 5e-5,\n",
    "                              eps = 1e-08\n",
    "                              )\n",
    "\n",
    "# Run on GPU\n",
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VA5Zgxk9pvLt",
    "outputId": "4d8ce539-7b3f-42bd-ec99-f8bc2d642b43"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:   0%|                                                                                                                                                                                                                      | 0/3 [00:00<?, ?it/s]<timed exec>:15: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7699207a50054d3fbf5b85291b64f325",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6955 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  33%|███████████████████████████████████████████████████████████████████▎                                                                                                                                      | 1/3 [54:52<1:49:45, 3292.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\t - Train loss: 1.4259\n",
      "\t - Validation Accuracy: 0.0167\n",
      "\t - Validation Precision: 0.4444\n",
      "\t - Validation Recall: 0.8578\n",
      "\t - Validation Specificity: 0.2131\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c497aedf032549868a3cfe5653b5b1fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6955 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  67%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋                                                                   | 2/3 [1:49:09<54:31, 3271.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\t - Train loss: 0.8270\n",
      "\t - Validation Accuracy: 0.0184\n",
      "\t - Validation Precision: 0.5963\n",
      "\t - Validation Recall: 0.8767\n",
      "\t - Validation Specificity: 0.2844\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cfb6cdc723604ba6bbd4dc74ec8a23e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6955 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [2:45:01<00:00, 3300.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\t - Train loss: 0.5374\n",
      "\t - Validation Accuracy: 0.0196\n",
      "\t - Validation Precision: 0.6159\n",
      "\t - Validation Recall: 0.9546\n",
      "\t - Validation Specificity: 0.3642\n",
      "\n",
      "Wall time: 2h 45min 1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Recommended number of epochs: 2, 3, 4. See: https://arxiv.org/pdf/1810.04805.pdf\n",
    "epochs = 3\n",
    "\n",
    "for _ in trange(epochs, desc = 'Epoch'):\n",
    "    \n",
    "    # ========== Training ==========\n",
    "    \n",
    "    # Set model to training mode\n",
    "    model.train()\n",
    "    \n",
    "    # Tracking variables\n",
    "    tr_loss = 0\n",
    "    nb_tr_examples, nb_tr_steps = 0, 0\n",
    "\n",
    "    for step, batch in enumerate(tqdm_notebook(train_dataloader)):\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "        optimizer.zero_grad()\n",
    "        # Forward pass\n",
    "        train_output = model(b_input_ids, \n",
    "                             token_type_ids = None, \n",
    "                             attention_mask = b_input_mask, \n",
    "                             labels = b_labels)\n",
    "        # Backward pass\n",
    "        train_output.loss.backward()\n",
    "        optimizer.step()\n",
    "        # Update tracking variables\n",
    "        tr_loss += train_output.loss.item()\n",
    "        nb_tr_examples += b_input_ids.size(0)\n",
    "        nb_tr_steps += 1\n",
    "\n",
    "    # ========== Validation ==========\n",
    "\n",
    "    # Set model to evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    # Tracking variables \n",
    "    val_accuracy = []\n",
    "    val_precision = []\n",
    "    val_recall = []\n",
    "    val_specificity = []\n",
    "\n",
    "    for batch in validation_dataloader:\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "        with torch.no_grad():\n",
    "          # Forward pass\n",
    "          eval_output = model(b_input_ids, \n",
    "                              token_type_ids = None, \n",
    "                              attention_mask = b_input_mask)\n",
    "        logits = eval_output.logits.detach().cpu().numpy()\n",
    "        label_ids = b_labels.to('cpu').numpy()\n",
    "        # Calculate validation metrics\n",
    "        b_accuracy, b_precision, b_recall, b_specificity = b_metrics(logits, label_ids)\n",
    "        val_accuracy.append(b_accuracy)\n",
    "        # Update precision only when (tp + fp) !=0; ignore nan\n",
    "        if b_precision != 'nan': val_precision.append(b_precision)\n",
    "        # Update recall only when (tp + fn) !=0; ignore nan\n",
    "        if b_recall != 'nan': val_recall.append(b_recall)\n",
    "        # Update specificity only when (tn + fp) !=0; ignore nan\n",
    "        if b_specificity != 'nan': val_specificity.append(b_specificity)\n",
    "\n",
    "    print('\\n\\t - Train loss: {:.4f}'.format(tr_loss / nb_tr_steps))\n",
    "    print('\\t - Validation Accuracy: {:.4f}'.format(sum(val_accuracy)/len(val_accuracy)))\n",
    "    print('\\t - Validation Precision: {:.4f}'.format(sum(val_precision)/len(val_precision)) if len(val_precision)>0 else '\\t - Validation Precision: NaN')\n",
    "    print('\\t - Validation Recall: {:.4f}'.format(sum(val_recall)/len(val_recall)) if len(val_recall)>0 else '\\t - Validation Recall: NaN')\n",
    "    print('\\t - Validation Specificity: {:.4f}\\n'.format(sum(val_specificity)/len(val_specificity)) if len(val_specificity)>0 else '\\t - Validation Specificity: NaN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "8r9ZN1_vpzdS"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Sentence:  a fantasy movie\n",
      "Predicted Class:  12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2364: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "def predict(sentence):\n",
    "    # We need Token IDs and Attention Mask for inference on the new sentence\n",
    "    test_ids = []\n",
    "    test_attention_mask = []\n",
    "\n",
    "    # Apply the tokenizer\n",
    "    encoding = preprocessing(new_sentence, tokenizer)\n",
    "\n",
    "    # Extract IDs and Attention Mask\n",
    "    test_ids.append(encoding['input_ids'])\n",
    "    test_attention_mask.append(encoding['attention_mask'])\n",
    "    test_ids = torch.cat(test_ids, dim = 0)\n",
    "    test_attention_mask = torch.cat(test_attention_mask, dim = 0)\n",
    "\n",
    "    # Forward pass, calculate logit predictions\n",
    "    with torch.no_grad():\n",
    "      output = model(test_ids.to(device), token_type_ids = None, attention_mask = test_attention_mask.to(device))\n",
    "\n",
    "    prediction = np.argmax(output.logits.cpu().numpy()).flatten().item()\n",
    "\n",
    "    print('Input Sentence: ', new_sentence)\n",
    "    print('Predicted Class: ', prediction)\n",
    "\n",
    "\n",
    "new_sentence = 'a fantasy movie'\n",
    "predict(new_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = './' \n",
    "torch.save(model, PATH + 'BERT_movie50_1.pt')  # 전체 모델 저장\n",
    "torch.save(model.state_dict(), PATH + 'model_state_dict50_1.pt')  # 모델 객체의 state_dict 저장"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
